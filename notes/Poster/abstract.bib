@Article{Lukosevicius_2009,
  Title                    = {{Reservoir computing approaches to recurrent neural network training}},
  Author                   = {Mantas Lukoševičius and Herbert Jaeger},
  Journal                  = {Computer Science Review},
  Year                     = {2009},
  Number                   = {3},
  Pages                    = {127 - 149},
  Volume                   = {3},

  Abstract                 = {Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current “brand-names” of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed “map” of it.},
  Doi                      = {https://doi.org/10.1016/j.cosrev.2009.03.005},
  File                     = {:/home/fschubert/papers/Lukosevicius_Jaeger-Reservoir_Computing_Approaches_to_Recurrent_Neural_Network_Training.pdf:PDF},
  ISSN                     = {1574-0137},
  Owner                    = {fschubert},
  Timestamp                = {2018.07.20},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1574013709000173}
}

@InProceedings{Caluwaerts_2013,
  Title                    = {The spectral radius remains a valid indicator of the echo state property for large reservoirs},
  Author                   = {Caluwaerts, Ken and wyffels, Francis and Dieleman, Sander and Schrauwen, Benjamin},
  Booktitle                = {IEEE International Joint Conference on Neural Networks (IJCNN)},
  Year                     = {2013},
  Pages                    = {6},

  Abstract                 = {In the field of Reservoir Computing, scaling the spectral radius of the weight matrix of a random recurrent neural network to below unity is a commonly used method to ensure the Echo State Property. Recently it has been shown that this condition is too weak. To overcome this problem, other more involved - sufficient conditions for the Echo State Property have been proposed. In this paper we provide a large-scale experimental verification of the Echo State Property for large recurrent neural networks with zero input and zero bias. Our main conclusion is that the spectral radius method remains a valid indicator of the Echo State Property; the probability that the Echo State Property does not hold, drops for larger networks with spectral radius below unity, which are the ones of practical interest.},
  ISBN                     = {9781467361293},
  ISSN                     = {2161-4393},
  Language                 = {eng},
  Location                 = {Dallas, Texas, USA}
}