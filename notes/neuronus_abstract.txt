Echo state networks have proven to be a powerful tool in the field of time series prediction. Several approaches to the optimization of the dynamic reservoir have been investigated in the past, including global tuning for criticality ,as well as local adaptation towards a given output distribution. The spectral radius of the synaptic weight matrix provides a measure to regulate the network in an appropriate working regime. We show that the spectral radius can be regulated by local homeostasis of the variance of neural activity. This variance control operates on a gain factor acting on the recurrent neural input and its optimization target depends on the variance of external inputs: we derived a suitable relation between recurrent and external input variance, that defines a manifold in the space of external and recurrent input variances. While our analytically derived condition is only exact for statistically independent external driving across nodes, numerical experiments suggest that it also holds under less restrictive conditions. This optimization rule is biologically plausible since it only relies on locally available information. In contrast to previously proposed optimization rules via local intrinsic plasticity, our model relies on the assumption that external and recurrent input signals can be treated as two separate streams of information. The network can hence react autonomously to changes of the input statistics. We demonstrate the importance of this separation by means of network performanceâ€”quantified by a nonlinear memory recall task.