% This file was created with JabRef 2.10.
% Encoding: UTF-8


@Article{Boedecker_2009,
  Title                    = {{Initialization and self-organized optimization of recurrent neural network connectivity}},
  Author                   = {Boedecker, J. and Obst, O. and Mayer, N. M. and Asada, M.},
  Journal                  = {{HFSP} Journal},
  Year                     = {2009},

  Month                    = {oct},
  Number                   = {5},
  Pages                    = {340--349},
  Volume                   = {3},

  Doi                      = {10.2976/1.3240502},
  File                     = {:/home/fschubert/papers/Boedecker_Obst_Mayer_Asada-Initizialization_and_self-organized_optimization_of_recurrent_neural_network_connectivity.pdf:PDF},
  Publisher                = {Informa {UK} Limited}
}

@InProceedings{Caluwaerts2013,
  Title                    = {{The spectral radius remains a valid indicator of the echo state property for large reservoirs}},
  Author                   = {Caluwaerts, Ken and wyffels, Francis and Dieleman, Sander and Schrauwen, Benjamin},
  Booktitle                = {IEEE International Joint Conference on Neural Networks (IJCNN)},
  Year                     = {2013},
  Pages                    = {6},

  Abstract                 = {In the field of Reservoir Computing, scaling the spectral radius of the weight matrix of a random recurrent neural network to below unity is a commonly used method to ensure the Echo State Property. Recently it has been shown that this condition is too weak. To overcome this problem, other more involved - sufficient conditions for the Echo State Property have been proposed. In this paper we provide a large-scale experimental verification of the Echo State Property for large recurrent neural networks with zero input and zero bias. Our main conclusion is that the spectral radius method remains a valid indicator of the Echo State Property; the probability that the Echo State Property does not hold, drops for larger networks with spectral radius below unity, which are the ones of practical interest.},
  ISBN                     = {9781467361293},
  ISSN                     = {2161-4393},
  Language                 = {eng},
  Location                 = {Dallas, Texas, USA}
}

@InProceedings{Caluwaerts_2013,
  Title                    = {{The spectral radius remains a valid indicator of the echo state property for large reservoirs}},
  Author                   = {Caluwaerts, Ken and Wyffels, Francis and Dieleman, Sander and Schrauwen, Benjamin},
  Booktitle                = {IEEE International Joint Conference on Neural Networks (IJCNN)},
  Year                     = {2013},
  Pages                    = {6},

  Abstract                 = {In the field of Reservoir Computing, scaling the spectral radius of the weight matrix of a random recurrent neural network to below unity is a commonly used method to ensure the Echo State Property. Recently it has been shown that this condition is too weak. To overcome this problem, other more involved - sufficient conditions for the Echo State Property have been proposed. In this paper we provide a large-scale experimental verification of the Echo State Property for large recurrent neural networks with zero input and zero bias. Our main conclusion is that the spectral radius method remains a valid indicator of the Echo State Property; the probability that the Echo State Property does not hold, drops for larger networks with spectral radius below unity, which are the ones of practical interest.},
  File                     = {:/home/fschubert/papers/Caluwaerts_Wyffels_Dieleman_Schrauwen-The_Spectral_Radius_Remains_a_Valid_Indicator_of_the_Echo_State_Property_for_Large_Reservoirs.pdf:PDF},
  ISBN                     = {9781467361293},
  ISSN                     = {2161-4393},
  Language                 = {eng},
  Location                 = {Dallas, Texas, USA}
}

@InProceedings{Jaeger_2010,
  Title                    = {{The ``echo state” approach to analysing and training recurrent neural networks – with an Erratum note}},
  Author                   = {Herbert Jaeger},
  Year                     = {2010},

  Owner                    = {fschubert},
  Timestamp                = {2018.07.20}
}

@TechReport{Jaeger_2001,
  Title                    = {{The "echo state" approach to analysing and training recurrent neural networks}},
  Author                   = {Jaeger, H.},
  Institution              = {GMD - German National Research Institute for Computer Science},
  Year                     = {2001},
  Number                   = {148},
  Type                     = {GMD Report},

  Added-at                 = {2008-03-11T14:52:34.000+0100},
  Biburl                   = {https://www.bibsonomy.org/bibtex/23d434b04cf1479acf45be9af65f8bc78/idsia},
  Interhash                = {c24261c15e88abe24a39d13339cc7697},
  Intrahash                = {3d434b04cf1479acf45be9af65f8bc78},
  Keywords                 = {imported},
  Timestamp                = {2008-03-11T14:52:36.000+0100},
  Url                      = {http://www.faculty.jacobs-university.de/hjaeger/pubs/EchoStatesTechRep.pdf}
}

@Article{Livi_2016,
  Title                    = {{Determination of the edge of cricritical in echo state networks through Fisher information maximization}},
  Author                   = {Livi, L. and Bianchi, F. M. and Alippi, C.},
  Journal                  = {arXiv:1603.03685v2},
  Year                     = {2016},

  Owner                    = {fschubert},
  Timestamp                = {2019.02.12}
}

@Article{Lukosevicius_2009,
  Title                    = {{Reservoir computing approaches to recurrent neural network training}},
  Author                   = {Mantas Lukoševičius and Herbert Jaeger},
  Journal                  = {Computer Science Review},
  Year                     = {2009},
  Number                   = {3},
  Pages                    = {127 - 149},
  Volume                   = {3},

  Abstract                 = {Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current “brand-names” of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed “map” of it.},
  Doi                      = {https://doi.org/10.1016/j.cosrev.2009.03.005},
  File                     = {:/home/fschubert/papers/Lukosevicius_Jaeger-Reservoir_Computing_Approaches_to_Recurrent_Neural_Network_Training.pdf:PDF},
  ISSN                     = {1574-0137},
  Owner                    = {fschubert},
  Timestamp                = {2018.07.20},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1574013709000173}
}

@Article{Schrauwen_2008,
  Title                    = {{Improving reservoirs using intrinsic plasticity}},
  Author                   = {Benjamin Schrauwen and Marion Wardermann and David Verstraeten and Jochen J. Steil and Dirk Stroobandt},
  Journal                  = {Neurocomputing},
  Year                     = {2008},

  Month                    = {mar},
  Number                   = {7-9},
  Pages                    = {1159--1171},
  Volume                   = {71},

  Doi                      = {10.1016/j.neucom.2007.12.020},
  Publisher                = {Elsevier {BV}}
}

@Article{Wainrib_2016,
  Title                    = {{A local Echo State Property through the largest Lyapunov exponent}},
  Author                   = {Gilles Wainrib and Mathieu N. Galtier},
  Journal                  = {Neural Networks},
  Year                     = {2016},
  Pages                    = {39 - 45},
  Volume                   = {76},

  Abstract                 = {Echo State Networks are efficient time-series predictors, which highly depend on the value of the spectral radius of the reservoir connectivity matrix. Based on recent results on the mean field theory of driven random recurrent neural networks, enabling the computation of the largest Lyapunov exponent of an ESN, we develop a cheap algorithm to establish a local and operational version of the Echo State Property.},
  Doi                      = {https://doi.org/10.1016/j.neunet.2015.12.013},
  ISSN                     = {0893-6080},
  Keywords                 = {Reservoir computing, Mean field theory, Lyapunov exponents, Echo State Networks},
  Owner                    = {fschubert},
  Timestamp                = {2019.02.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0893608015002828}
}

@Article{Yildiz_2012,
  Title                    = {{Re-visiting the echo state property}},
  Author                   = {Izzet B. Yildiz and Herbert Jaeger and Stefan J. Kiebel},
  Journal                  = {Neural Networks},
  Year                     = {2012},
  Pages                    = {1 - 9},
  Volume                   = {35},

  Abstract                 = {An echo state network (ESN) consists of a large, randomly connected neural network, the reservoir, which is driven by an input signal and projects to output units. During training, only the connections from the reservoir to these output units are learned. A key requisite for output-only training is the echo state property (ESP), which means that the effect of initial conditions should vanish as time passes. In this paper, we use analytical examples to show that a widely used criterion for the ESP, the spectral radius of the weight matrix being smaller than unity, is not sufficient to satisfy the echo state property. We obtain these examples by investigating local bifurcation properties of the standard ESNs. Moreover, we provide new sufficient conditions for the echo state property of standard sigmoid and leaky integrator ESNs. We furthermore suggest an improved technical definition of the echo state property, and discuss what practicians should (and should not) observe when they optimize their reservoirs for specific tasks.},
  Doi                      = {https://doi.org/10.1016/j.neunet.2012.07.005},
  ISSN                     = {0893-6080},
  Keywords                 = {Echo state network, Spectral radius, Bifurcation, Diagonally Schur stable, Lyapunov},
  Owner                    = {fschubert},
  Timestamp                = {2019.02.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0893608012001852}
}

