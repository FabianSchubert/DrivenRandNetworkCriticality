% Encoding: UTF-8


@Article{arviv2015near,
  Title                    = {Near-critical dynamics in stimulus-evoked activity of the human brain and its relation to spontaneous resting-state activity},
  Author                   = {Arviv, Oshrit and Goldstein, Abraham and Shriki, Oren},
  Journal                  = {Journal of Neuroscience},
  Year                     = {2015},
  Number                   = {41},
  Pages                    = {13927--13942},
  Volume                   = {35},

  Publisher                = {Soc Neuroscience}
}

@Article{berkes2011spontaneous,
  Title                    = {Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment},
  Author                   = {Berkes, Pietro and Orb{\'a}n, Gerg{\H{o}} and Lengyel, M{\'a}t{\'e} and Fiser, J{\'o}zsef},
  Journal                  = {Science},
  Year                     = {2011},
  Number                   = {6013},
  Pages                    = {83--87},
  Volume                   = {331},

  Publisher                = {American Association for the Advancement of Science}
}

@Article{boedecker2012information,
  Title                    = {Information processing in echo state networks at the edge of chaos},
  Author                   = {Boedecker, Joschka and Obst, Oliver and Lizier, Joseph T and Mayer, N Michael and Asada, Minoru},
  Journal                  = {Theory in Biosciences},
  Year                     = {2012},
  Number                   = {3},
  Pages                    = {205--213},
  Volume                   = {131},

  Publisher                = {Springer}
}

@Article{boedecker2009initialization,
  Title                    = {Initialization and self-organized optimization of recurrent neural network connectivity},
  Author                   = {Boedecker, Joschka and Obst, Oliver and Mayer, N Michael and Asada, Minoru},
  Journal                  = {HFSP journal},
  Year                     = {2009},
  Number                   = {5},
  Pages                    = {340--349},
  Volume                   = {3},

  Publisher                = {Taylor \& Francis}
}

@InProceedings{caluwaerts2013spectral,
  Title                    = {The spectral radius remains a valid indicator of the echo state property for large reservoirs},
  Author                   = {Caluwaerts, Ken and Wyffels, Francis and Dieleman, Sander and Schrauwen, Benjamin},
  Booktitle                = {Neural Networks (IJCNN), The 2013 International Joint Conference on},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {1--6}
}

@Article{cannon2017stable,
  Title                    = {Stable Control of Firing Rate Mean and Variance by Dual Homeostatic Mechanisms},
  Author                   = {Cannon, Jonathan and Miller, Paul},
  Journal                  = {The Journal of Mathematical Neuroscience},
  Year                     = {2017},
  Number                   = {1},
  Pages                    = {1},
  Volume                   = {7},

  Publisher                = {Springer}
}

@Article{caraballo2006pullback,
  Title                    = {Pullback attractors for asymptotically compact non-autonomous dynamical systems},
  Author                   = {Caraballo, Tom{\'a}s and {\L}ukaszewicz, Grzegorz and Real, Jos{\'e}},
  Journal                  = {Nonlinear Analysis: Theory, Methods \& Applications},
  Year                     = {2006},
  Number                   = {3},
  Pages                    = {484--498},
  Volume                   = {64},

  Publisher                = {Elsevier}
}

@Article{cocchi2017criticality,
  Title                    = {Criticality in the brain: A synthesis of neurobiology, models and cognition},
  Author                   = {Cocchi, Luca and Gollo, Leonardo L and Zalesky, Andrew and Breakspear, Michael},
  Journal                  = {Progress in neurobiology},
  Year                     = {2017},
  Pages                    = {132--152},
  Volume                   = {158},

  Publisher                = {Elsevier}
}

@Article{Dieci_1995,
  Title                    = {{Computation of a few Lyapunov exponents for continuous and discrete dynamical systems}},
  Author                   = {Dieci, L. and Van Vleck, E. S.},
  Journal                  = {Applied Numerical Mathematics},
  Year                     = {1995},
  Pages                    = {275--291},
  Volume                   = {17},

  File                     = {:/home/fschubert/papers/Dieci_Van_Vleck-Compuation_of_a_few_Lyapunov_exponents_for_continuous_and_discrete_dynamical_systems.pdf:PDF},
  Owner                    = {fschubert},
  Timestamp                = {2019.04.12}
}

@Article{echeveste2014generating,
  Title                    = {Generating functionals for computational intelligence: the Fisher information as an objective function for self-limiting Hebbian learning rules},
  Author                   = {Echeveste, Rodrigo and Gros, Claudius},
  Journal                  = {Frontiers in Robotics and AI},
  Year                     = {2014},
  Pages                    = {1},
  Volume                   = {1},

  Publisher                = {Frontiers}
}

@Article{enel2016reservoir,
  Title                    = {Reservoir computing properties of neural dynamics in prefrontal cortex},
  Author                   = {Enel, Pierre and Procyk, Emmanuel and Quilodran, Ren{\'e} and Dominey, Peter Ford},
  Journal                  = {PLoS computational biology},
  Year                     = {2016},
  Number                   = {6},
  Pages                    = {e1004967},
  Volume                   = {12},

  Publisher                = {Public Library of Science}
}

@Article{farkavs2016computational,
  Title                    = {Computational analysis of memory capacity in echo state networks},
  Author                   = {Farka{\v{s}}, Igor and Bos{\'a}k, Radom{\'\i}r and Gergel', Peter},
  Journal                  = {Neural Networks},
  Year                     = {2016},
  Pages                    = {109--120},
  Volume                   = {83},

  Publisher                = {Elsevier}
}

@Article{gallicchio2017echo,
  Title                    = {Echo state property of deep reservoir computing networks},
  Author                   = {Gallicchio, Claudio and Micheli, Alessio},
  Journal                  = {Cognitive Computation},
  Year                     = {2017},
  Number                   = {3},
  Pages                    = {337--350},
  Volume                   = {9},

  Publisher                = {Springer}
}

@Book{gros2015complex,
  Title                    = {Complex and adaptive dynamical systems: A primer},
  Author                   = {Gros, C.},
  Publisher                = {Springer},
  Year                     = {2015}
}

@Article{gros2009cognitive,
  Title                    = {Cognitive computation with autonomously active neural networks: an emerging field},
  Author                   = {Gros, Claudius},
  Journal                  = {Cognitive Computation},
  Year                     = {2009},
  Number                   = {1},
  Pages                    = {77--90},
  Volume                   = {1},

  Publisher                = {Springer}
}

@Article{gros1990criterion,
  Title                    = {Criterion for a good variational wave function},
  Author                   = {Gros, Claudius},
  Journal                  = {Physical Review B},
  Year                     = {1990},
  Number                   = {10},
  Pages                    = {6835},
  Volume                   = {42},

  Publisher                = {APS}
}

@InProceedings{jaeger2005reservoir,
  Title                    = {Reservoir riddles: Suggestions for echo state network research},
  Author                   = {Jaeger, Herbert},
  Booktitle                = {Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.},
  Year                     = {2005},
  Organization             = {IEEE},
  Pages                    = {1460--1462},
  Volume                   = {3}
}

@TechReport{Jaeger_2002,
  Title                    = {{Short Term Memory in Echo State Networks}},
  Author                   = {Jaeger, H.},
  Institution              = {Fraunhofer Institute for Autonomous Intelligent Systems},
  Year                     = {2002},
  Number                   = {152},
  Type                     = {GMD Report},

  Owner                    = {fschubert},
  Timestamp                = {2019.04.17}
}

@Book{jaeger2002tutorial,
  Title                    = {Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the" echo state network" approach},
  Author                   = {Jaeger, Herbert},
  Publisher                = {GMD-Forschungszentrum Informationstechnik Bonn},
  Year                     = {2002},
  Volume                   = {5}
}

@TechReport{Jaeger_2001,
  Title                    = {{The "echo state" approach to analysing and training recurrent neural networks}},
  Author                   = {Jaeger, H.},
  Institution              = {GMD - German National Research Institute for Computer Science},
  Year                     = {2001},
  Number                   = {148},
  Type                     = {GMD Report},

  Added-at                 = {2008-03-11T14:52:34.000+0100},
  Biburl                   = {https://www.bibsonomy.org/bibtex/23d434b04cf1479acf45be9af65f8bc78/idsia},
  Interhash                = {c24261c15e88abe24a39d13339cc7697},
  Intrahash                = {3d434b04cf1479acf45be9af65f8bc78},
  Keywords                 = {imported},
  Timestamp                = {2008-03-11T14:52:36.000+0100},
  Url                      = {http://www.faculty.jacobs-university.de/hjaeger/pubs/EchoStatesTechRep.pdf}
}

@Article{kloeden2000pullback,
  Title                    = {Pullback attractors in nonautonomous difference equations},
  Author                   = {Kloeden, Peter E},
  Journal                  = {Journal of Difference Equations and Applications},
  Year                     = {2000},
  Number                   = {1},
  Pages                    = {33--52},
  Volume                   = {6},

  Publisher                = {Taylor \& Francis}
}

@Article{linkerhand2013self,
  Title                    = {Self-organized stochastic tipping in slow-fast dynamical systems},
  Author                   = {Linkerhand, Mathias and Gros, Claudius},
  Journal                  = {Mathematics and Mechanics of Complex Systems},
  Year                     = {2013},
  Number                   = {2},
  Pages                    = {129--147},
  Volume                   = {1},

  Publisher                = {Mathematical Sciences Publishers}
}

@Article{livi2018determination,
  Title                    = {Determination of the edge of criticality in echo state networks through Fisher information maximization},
  Author                   = {Livi, Lorenzo and Bianchi, Filippo Maria and Alippi, Cesare},
  Journal                  = {IEEE Transactions on Neural Networks and Learning Systems},
  Year                     = {2018},
  Number                   = {3},
  Pages                    = {706--717},
  Volume                   = {29},

  Publisher                = {IEEE}
}

@Article{lukovsevivcius2009reservoir,
  Title                    = {Reservoir computing approaches to recurrent neural network training},
  Author                   = {Luko{\v{s}}evi{\v{c}}ius, Mantas and Jaeger, Herbert},
  Journal                  = {Computer Science Review},
  Year                     = {2009},
  Number                   = {3},
  Pages                    = {127--149},
  Volume                   = {3},

  Publisher                = {Elsevier}
}

@Article{maass2002real,
  Title                    = {Real-time computing without stable states: A new framework for neural computation based on perturbations},
  Author                   = {Maass, Wolfgang and Natschl{\"a}ger, Thomas and Markram, Henry},
  Journal                  = {Neural computation},
  Year                     = {2002},
  Number                   = {11},
  Pages                    = {2531--2560},
  Volume                   = {14},

  Publisher                = {MIT Press}
}

@Article{manjunath2013echo,
  Title                    = {Echo state property linked to an input: Exploring a fundamental characteristic of recurrent neural networks},
  Author                   = {Manjunath, Gandhi and Jaeger, Herbert},
  Journal                  = {Neural computation},
  Year                     = {2013},
  Number                   = {3},
  Pages                    = {671--696},
  Volume                   = {25},

  Publisher                = {MIT Press}
}

@Article{marder2006variability,
  Title                    = {Variability, compensation and homeostasis in neuron and network function},
  Author                   = {Marder, Eve and Goaillard, Jean-Marc},
  Journal                  = {Nature Reviews Neuroscience},
  Year                     = {2006},
  Number                   = {7},
  Pages                    = {563},
  Volume                   = {7},

  Publisher                = {Nature Publishing Group}
}

@Article{markovic2010self,
  Title                    = {Self-organized chaos through polyhomeostatic optimization},
  Author                   = {Markovic, Dimitrije and Gros, Claudius},
  Journal                  = {Physical Review Letters},
  Year                     = {2010},
  Number                   = {6},
  Pages                    = {068702},
  Volume                   = {105},

  Publisher                = {APS}
}

@Article{mitra2016networks,
  Title                    = {How networks communicate: propagation patterns in spontaneous brain activity},
  Author                   = {Mitra, Anish and Raichle, Marcus E},
  Journal                  = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  Year                     = {2016},
  Number                   = {1705},
  Pages                    = {20150546},
  Volume                   = {371},

  Publisher                = {The Royal Society}
}

@Article{nikolic2009distributed,
  Title                    = {Distributed fading memory for stimulus properties in the primary visual cortex},
  Author                   = {Nikoli{\'c}, Danko and H{\"a}usler, Stefan and Singer, Wolf and Maass, Wolfgang},
  Journal                  = {PLoS biology},
  Year                     = {2009},
  Number                   = {12},
  Pages                    = {e1000260},
  Volume                   = {7},

  Publisher                = {Public Library of Science}
}

@Article{ozturk2007analysis,
  Title                    = {Analysis and design of echo state networks},
  Author                   = {Ozturk, Mustafa C and Xu, Dongming and Pr{\'\i}ncipe, Jos{\'e} C},
  Journal                  = {Neural computation},
  Year                     = {2007},
  Number                   = {1},
  Pages                    = {111--138},
  Volume                   = {19},

  Publisher                = {MIT Press}
}

@Article{petermann2009spontaneous,
  Title                    = {Spontaneous cortical activity in awake monkeys composed of neuronal avalanches},
  Author                   = {Petermann, Thomas and Thiagarajan, Tara C and Lebedev, Mikhail A and Nicolelis, Miguel AL and Chialvo, Dante R and Plenz, Dietmar},
  Journal                  = {Proceedings of the National Academy of Sciences},
  Year                     = {2009},
  Number                   = {37},
  Pages                    = {15921--15926},
  Volume                   = {106},

  Publisher                = {National Acad Sciences}
}

@Article{schrauwen2008improving,
  Title                    = {Improving reservoirs using intrinsic plasticity},
  Author                   = {Schrauwen, Benjamin and Wardermann, Marion and Verstraeten, David and Steil, Jochen J and Stroobandt, Dirk},
  Journal                  = {Neurocomputing},
  Year                     = {2008},
  Number                   = {7-9},
  Pages                    = {1159--1171},
  Volume                   = {71},

  Publisher                = {Elsevier}
}

@Article{schuecker2018optimal,
  Title                    = {Optimal sequence memory in driven random networks},
  Author                   = {Schuecker, Jannis and Goedeke, Sven and Helias, Moritz},
  Journal                  = {Physical Review X},
  Year                     = {2018},
  Number                   = {4},
  Pages                    = {041029},
  Volume                   = {8},

  Publisher                = {APS}
}

@Article{sompolinsky1988chaos,
  Title                    = {Chaos in random neural networks},
  Author                   = {Sompolinsky, Haim and Crisanti, Andrea and Sommers, Hans-Jurgen},
  Journal                  = {Physical review letters},
  Year                     = {1988},
  Number                   = {3},
  Pages                    = {259},
  Volume                   = {61},

  Publisher                = {APS}
}

@InProceedings{triesch2005gradient,
  Title                    = {A gradient rule for the plasticity of a neuron’s intrinsic excitability},
  Author                   = {Triesch, Jochen},
  Booktitle                = {International Conference on Artificial Neural Networks},
  Year                     = {2005},
  Organization             = {Springer},
  Pages                    = {65--70}
}

@Article{wainrib2016local,
  Title                    = {A local Echo State Property through the largest Lyapunov exponent},
  Author                   = {Wainrib, Gilles and Galtier, Mathieu N},
  Journal                  = {Neural Networks},
  Year                     = {2016},
  Pages                    = {39--45},
  Volume                   = {76},

  Publisher                = {Elsevier}
}

@Article{wernecke2019chaos,
  Title                    = {Chaos in time delay systems, an educational review},
  Author                   = {Wernecke, Hendrik and S{\'a}ndor, Bulcs{\'u} and Gros, Claudius},
  Journal                  = {arXiv preprint arXiv:1901.04826},
  Year                     = {2019}
}

@Article{yildiz2012re,
  Title                    = {Re-visiting the echo state property},
  Author                   = {Yildiz, Izzet B and Jaeger, Herbert and Kiebel, Stefan J},
  Journal                  = {Neural networks},
  Year                     = {2012},
  Pages                    = {1--9},
  Volume                   = {35},

  Publisher                = {Elsevier}
}

@Article{Rajan2006,
  author    = {Kanaka Rajan and L. F. Abbott},
  title     = {Eigenvalue Spectra of Random Matrices for Neural Networks},
  journal   = {Physical Review Letters},
  year      = {2006},
  volume    = {97},
  number    = {18},
  month     = {nov},
  doi       = {10.1103/physrevlett.97.188104},
  publisher = {American Physical Society ({APS})},
}

@Article{Froemke_2005,
  author    = {Froemke, R. C. and Poo, M. and Dan, Y.},
  title     = {{Spike-timing-dependent synaptic plasticity depends on dendritic location}},
  journal   = {Nature},
  year      = {2005},
  volume    = {434},
  number    = {7030},
  pages     = {221--225},
  month     = mar,
  issn      = {0028-0836},
  comment   = {10.1038/nature03366},
  doi       = {10.1038/nature03366},
  owner     = {fschubert},
  timestamp = {2016.10.06},
}

@Article{Abbott_2000,
  author    = {Abbott, L. and Nelson, S. B.},
  title     = {{Synaptic Plasticity: taming the beast}},
  journal   = {Nature Neuroscience},
  year      = {2000},
  owner     = {fschubert},
  timestamp = {2016.12.20},
}

@Misc{Powerlaw_Package,
  author       = {Alstott, J.},
  title        = {{powerlaw: A Python Package for Analysis of Heavy-Tailed Distributions (Version 1.4.1) [Computer Software]}},
  howpublished = {\url{https://pypi.python.org/pypi/powerlaw}},
  year         = {2017},
  owner        = {fschubert},
  timestamp    = {2016.10.06},
}

@Article{Azouz_2000,
  author    = {Azouz, R. and Gray, C. M.},
  title     = {{Dynamic spike threshold reveals a mechanism for synaptic coincidence detection in cortical neurons in vivo}},
  journal   = {Proceedings of the National Academy of Sciences of the United States of America},
  year      = {2000},
  volume    = {97},
  number    = {14},
  pages     = {8110--8115},
  month     = may,
  issn      = {1091-6490},
  abstract  = {Cortical neurons are sensitive to the timing of their synaptic inputs. They can synchronize their firing on a millisecond time scale and follow rapid stimulus fluctuations with high temporal precision. These findings suggest that cortical neurons have an enhanced sensitivity to synchronous synaptic inputs that lead to rapid rates of depolarization. The voltage-gated currents underlying action potential generation may provide one mechanism to amplify rapid depolarizations. We have tested this hypothesis by analyzing the relations between membrane potential fluctuations and spike threshold in cat visual cortical neurons recorded intracellularly in vivo. We find that visual stimuli evoke broad variations in spike threshold that are caused in large part by an inverse relation between spike threshold and the rate of membrane depolarization preceding a spike. We also find that spike threshold is inversely related to the rate of rise of the action potential upstroke, suggesting that increases in spike threshold result from a decrease in the availability of Na(+) channels. By using a simple neuronal model, we show that voltage-gated Na(+) and K(+) conductances endow cortical neurons with an enhanced sensitivity to rapid depolarizations that arise from synchronous excitatory synaptic inputs. Thus, the basic mechanism responsible for action potential generation also enhances the sensitivity of cortical neurons to coincident synaptic inputs.},
  comment   = {130200797[PII] 2007[PII] 10859358[pmid]},
  owner     = {fabian},
  publisher = {The National Academy of Sciences},
  timestamp = {2017.06.20},
  url       = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC16678/},
}

@Article{Barbas_2015,
  author    = {Barbas, H.},
  title     = {{General Cortical and Special Prefrontal Connections: Principles from Structure to Function}},
  journal   = {Annual Review of Neuroscience},
  year      = {2015},
  volume    = {38},
  number    = {1},
  pages     = {269--289},
  month     = {jul},
  doi       = {10.1146/annurev-neuro-071714-033936},
  publisher = {Annual Reviews},
}

@Article{Battaglia_2005,
  author    = {Battaglia, F. P. and Sutherland, G. R. and Cowen, S. L. and Mc Naughton, B. L. and Harris, K. D.},
  title     = {{Firing rate modulation: A simple statistical view of memory trace reactivation}},
  journal   = {Neural Networks},
  year      = {2005},
  volume    = {18},
  number    = {9},
  pages     = {1280--1291},
  issn      = {0893-6080},
  note      = {Computational Theories of the Functions of the Hippocampus},
  doi       = {10.1016/j.neunet.2005.08.011},
  keywords  = {Memory consolidation},
  owner     = {fabian},
  timestamp = {2017.05.09},
  url       = {http://www.sciencedirect.com/science/article/pii/S0893608005001954},
}

@Book{Bear_Exploring_the_Brain,
  title     = {{Neuroscience: Exploring the Brain}},
  publisher = {Lippincott Williams \& Wilki},
  year      = {2007},
  author    = {Bear, Mark F. and Connors, B. W. and Paradiso, Michael A.},
  editor    = {Emily Lupash},
  owner     = {fabian},
  timestamp = {2017.01.20},
}

@Article{Bell_1995,
  author    = {Bell, A. J. and Sejnowski, T. J.},
  title     = {{An Information-maximisation approach to blind separation and blind deconvolution}},
  journal   = {Neural Computation},
  year      = {1995},
  volume    = {7},
  pages     = {1129-1159},
  file      = {:/home/fschubert/papers/infomax_bell_sejnowski.pdf:PDF},
  owner     = {fschubert},
  timestamp = {2017.11.20},
}

@Article{Benda_2003,
  author    = {Benda, J. and Herz, A. V. M.},
  title     = {{A Universal Model for Spike-Frequency Adaption}},
  journal   = {Neural Computation},
  year      = {2003},
  volume    = {15},
  number    = {11},
  pages     = {2523--2564},
  owner     = {fschubert},
  timestamp = {2016.10.11},
}

@Article{Bi_1998,
  author   = {Bi, G. and Poo, M.},
  title    = {{Synaptic Modifications in Cultured Hippocampal Neurons: Dependence on Spike Timing, Synaptic Strength, and Postsynaptic Cell Type}},
  journal  = {The Journal of Neuroscience},
  year     = {1998},
  volume   = {18},
  number   = {24},
  pages    = {10464--10472},
  abstract = {In cultures of dissociated rat hippocampal neurons, persistent potentiation and depression of glutamatergic synapses were induced by correlated spiking of presynaptic and postsynaptic neurons. The relative timing between the presynaptic and postsynaptic spiking determined the direction and the extent of synaptic changes. Repetitive postsynaptic spiking within a time window of 20 msec after presynaptic activation resulted in long-term potentiation (LTP), whereas postsynaptic spiking within a window of 20 msec before the repetitive presynaptic activation led to long-term depression (LTD). Significant LTP occurred only at synapses with relatively low initial strength, whereas the extent of LTD did not show obvious dependence on the initial synaptic strength. Both LTP and LTD depended on the activation of NMDA receptors and were absent in cases in which the postsynaptic neurons were GABAergic in nature. Blockade of L-type calcium channels with nimodipine abolished the induction of LTD and reduced the extent of LTP. These results underscore the importance of precise spike timing, synaptic strength, and postsynaptic cell type in the activity-induced modification of central synapses and suggest that Hebb{\textquoteright}s rule may need to incorporate a quantitative consideration of spike timing that reflects the narrow and asymmetric window for the induction of synaptic modification.},
  eprint   = {http://www.jneurosci.org/content/18/24/10464.full.pdf+html},
  url      = {http://www.jneurosci.org/content/18/24/10464.abstract},
}

@Article{Boedecker_2009,
  author    = {Boedecker, J. and Obst, O. and Mayer, N. M. and Asada, M.},
  title     = {{Initialization and self-organized optimization of recurrent neural network connectivity}},
  journal   = {{HFSP} Journal},
  year      = {2009},
  volume    = {3},
  number    = {5},
  pages     = {340--349},
  month     = {oct},
  doi       = {10.2976/1.3240502},
  file      = {:/home/fschubert/papers/Boedecker_Obst_Mayer_Asada-Initizialization_and_self-organized_optimization_of_recurrent_neural_network_connectivity.pdf:PDF},
  publisher = {Informa {UK} Limited},
}

@Article{Bono_2017,
  author    = {Bono, J. and Clopath, C.},
  title     = {{Modeling somatic and dendritic spike mediated plasticity at the single neuron and network level}},
  journal   = {nature communications},
  year      = {2017},
  file      = {:/home/fschubert/papers/Bono_Clopath_Modeling_somatic_and_dendritic_spike_mediated_plasticity_at_the_single_neuron_and_network_level.pdf:PDF},
  owner     = {fschubert},
  timestamp = {2018.04.17},
}

@InCollection{Bourdoukan_2018,
  author    = {Bourdoukan, R. and Deneve, S.},
  title     = {{Enforcing balance allows local supervised learning in spiking recurrent networks}},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  year      = {2015},
  editor    = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages     = {982--990},
  url       = {http://papers.nips.cc/paper/5948-enforcing-balance-allows-local-supervised-learning-in-spiking-recurrent-networks.pdf},
}

@Article{Bredt_1990,
  author   = {Bredt, D. S. and Snyder, S. H.},
  title    = {{Isolation of nitric oxide synthetase, a calmodulin-requiring enzyme}},
  journal  = {Proceedings of the National Academy of Sciences},
  year     = {1990},
  volume   = {87},
  number   = {2},
  pages    = {682--685},
  abstract = {Nitric oxide mediates vascular relaxing effects of endothelial cells, cytotoxic actions of macrophages and neutrophils, and influences of excitatory amino acids on cerebellar cyclic GMP. Its enzymatic formation from arginine by a soluble enzyme associated with stoichiometric production of citrulline requires NADPH and Ca2+. We show that nitric oxide synthetase activity requires calmodulin. Utilizing a 2',5'-ADP affinity column eluted with NADPH, we have purified nitric oxide synthetase 6000-fold to homogeneity from rat cerebellum. The purified enzyme migrates as a single 150-kDa band on SDS/PAGE, and the native enzyme appears to be a monomer.},
  eprint   = {http://www.pnas.org/content/87/2/682.full.pdf},
  url      = {http://www.pnas.org/content/87/2/682.abstract},
}

@Article{Brette_2015,
  author   = {Brette, Romain},
  title    = {{Philosophy of the Spike: Rate-Based vs. Spike-Based Theories of the Brain}},
  journal  = {Frontiers in Systems Neuroscience},
  year     = {2015},
  volume   = {9},
  pages    = {151},
  issn     = {1662-5137},
  abstract = {Does the brain use a firing rate code or a spike timing code? Considering this controversial question from an epistemological perspective, I argue that progress has been hampered by its problematic phrasing. It takes the perspective of an external observer looking at whether those two observables vary with stimuli, and thereby misses the relevant question: which one has a causal role in neural activity? When rephrased in a more meaningful way, the rate-based view appears as an ad hoc methodological postulate, one that is practical but with virtually no empirical or theoretical support.},
  doi      = {10.3389/fnsys.2015.00151},
  file     = {:/home/fschubert/papers/Rate-Based_vs_Spike-Based_Theories.pdf:PDF},
  url      = {https://www.frontiersin.org/article/10.3389/fnsys.2015.00151},
}

@Misc{Briansim,
  author       = {Brette, Romain and Goodman, Dan and Stirnberg, Marcel},
  title        = {{The Brian spiking neural network simulator (Version 1.0) [Computer Software]}},
  howpublished = {\url{http://www.briansimulator.org/}},
  year         = {2016},
  owner        = {fschubert},
  timestamp    = {2016.10.06},
}

@Article{Brunel_2000,
  author   = {Brunel, N.},
  title    = {{Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons.}},
  journal  = {J Comput Neurosci},
  year     = {2000},
  volume   = {8},
  pages    = {183--208},
  abstract = {The dynamics of networks of sparsely connected excitatory and inhibitory integrate-and-fire neurons are studied analytically. The analysis reveals a rich repertoire of states, including synchronous states in which neurons fire regularly; asynchronous states with stationary global activity and very irregular individual cell activity; and states in which the global activity oscillates but individual cells fire irregularly, typically at rates lower than the global oscillation frequency. The network can switch between these states, provided the external frequency, or the balance between excitation and inhibition, is varied. Two types of network oscillations are observed. In the fast oscillatory state, the network frequency is almost fully controlled by the synaptic time scale. In the slow oscillatory state, the network frequency depends mostly on the membrane time constant. Finite size effects in the asynchronous state are also discussed.},
  file     = {:home/pierre/Mendeley/Brunel - 2000.pdf:pdf},
  keywords = {Action Potentials; Biological Clocks; Brain; Corti,Neurological; Nerve Net; Neural Inhibition; Neural},
  pmid     = {10809012},
}

@Article{Burrone_2003,
  author  = {Burrone, J. and Murthy, V. N.},
  title   = {{Synaptic gain control and homeostasis}},
  journal = {Current Opinion in Neurobiology},
  year    = {2003},
  volume  = {13},
  number  = {5},
  pages   = {560 - 567},
  issn    = {0959-4388},
  doi     = {http://dx.doi.org/10.1016/j.conb.2003.09.007},
  url     = {//www.sciencedirect.com/science/article/pii/S0959438803001284},
}

@Article{Burrone_2002,
  author    = {Burrone, J. and O'Byrne, M. and Murthy, V. N.},
  title     = {{Multiple forms of synaptic plasticity triggered by selective supperssion of activity}},
  journal   = {Nature},
  year      = {2002},
  owner     = {fschubert},
  timestamp = {2017.01.31},
}

@Article{Buzsaki_2004,
  author    = {Buzsáki, G. and Geisler, C. and Henze, D. A. and Wang, X.-J.},
  title     = {{Interneuron Diversity series: Circuit complexity and axon wiring economy of cortical interneurons}},
  journal   = {Trends in Neurosciences},
  year      = {2004},
  volume    = {27},
  owner     = {fabian},
  timestamp = {2017.03.01},
}

@Article{Buzsaki_2014,
  author    = {Buzsáki, G. and Mizuseki, K.},
  title     = {{The log-dynamic brain: how skewed distributions affect network operations}},
  journal   = {Nature Reviews Neuroscience},
  year      = {2014},
  owner     = {fabian},
  timestamp = {2016.12.11},
}

@InProceedings{Caluwaerts_2013,
  author    = {Caluwaerts, Ken and Wyffels, Francis and Dieleman, Sander and Schrauwen, Benjamin},
  title     = {{The spectral radius remains a valid indicator of the echo state property for large reservoirs}},
  booktitle = {IEEE International Joint Conference on Neural Networks (IJCNN)},
  year      = {2013},
  pages     = {6},
  abstract  = {In the field of Reservoir Computing, scaling the spectral radius of the weight matrix of a random recurrent neural network to below unity is a commonly used method to ensure the Echo State Property. Recently it has been shown that this condition is too weak. To overcome this problem, other more involved - sufficient conditions for the Echo State Property have been proposed. In this paper we provide a large-scale experimental verification of the Echo State Property for large recurrent neural networks with zero input and zero bias. Our main conclusion is that the spectral radius method remains a valid indicator of the Echo State Property; the probability that the Echo State Property does not hold, drops for larger networks with spectral radius below unity, which are the ones of practical interest.},
  file      = {:/home/fschubert/papers/Caluwaerts_Wyffels_Dieleman_Schrauwen-The_Spectral_Radius_Remains_a_Valid_Indicator_of_the_Echo_State_Property_for_Large_Reservoirs.pdf:PDF},
  isbn      = {9781467361293},
  issn      = {2161-4393},
  language  = {eng},
  location  = {Dallas, Texas, USA},
}

@Book{Wisdom_of_the_Body_Cannon,
  title     = {{The Wisdom of the Body}},
  publisher = {Norton \& Company},
  year      = {1932},
  author    = {Cannon, W. B.},
  owner     = {fabian},
  timestamp = {2017.01.20},
}

@Article{Clopath_2017,
  author    = {Clopath, C. and Bonhoeffer, T. and H\"ubener, M. and Rose, T.},
  title     = {{Variance and invariance of neuronal long-term representations}},
  journal   = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  year      = {2017},
  owner     = {fschubert},
  timestamp = {2017.11.01},
}

@Article{Clopath_Bono_2017,
  author    = {Clopath, C. and Bono, J.},
  title     = {{Modeling somatic and dendritic spike mediated plasticity at the single neuron and network level}},
  journal   = {Nature Communications},
  year      = {2017},
  owner     = {fschubert},
  timestamp = {2018.08.16},
}

@Article{Clopath_2010,
  author    = {Clopath, C. and Büsing, L. and Vasilaki, E. and Gerstner, W.},
  title     = {{Connectivity reflects coding: a model of voltage-based {STDP} with homeostasis}},
  journal   = {Nature Neuroscience},
  year      = {2010},
  volume    = {13},
  number    = {3},
  pages     = {344--352},
  month     = {jan},
  doi       = {10.1038/nn.2479},
  publisher = {Springer Nature},
}

@Article{Collins_2010,
  author    = {Collins, C. E. and Airey, D. C. and Young, N. A. and Leitch, D. B. Kaas, J. H.},
  title     = {{Neuron densities vary across and within cortical areas in primates}},
  journal   = {Proceedings of the National Academy of Sciences of the United States of America},
  year      = {2010},
  volume    = {107},
  owner     = {fabian},
  timestamp = {2017.04.05},
}

@Article{Connors_1990,
  author  = {Connors, B. W. and Gutnick, M. J.},
  title   = {{Intrinsic firing patterns of diverse neocortical neurons}},
  journal = {Trends in Neurosciences},
  year    = {1990},
  volume  = {13},
  number  = {3},
  pages   = {99 - 104},
  issn    = {0166-2236},
  doi     = {http://dx.doi.org/10.1016/0166-2236(90)90185-D},
  url     = {http://www.sciencedirect.com/science/article/pii/016622369090185D},
}

@Article{Couto_2013,
  author    = {Couto, R. T.},
  title     = {{Green's functions for the wave, Helmholtz and Poisson equations in a two-dimensional boundless domain}},
  journal   = {Revista Brasileira de Ensino Física},
  year      = {2013},
  owner     = {fschubert},
  timestamp = {2017.01.10},
}

@Book{Cybernetik_Systems_Cruse_2006,
  title     = {{Neural Networks as Cybernetic Systems}},
  publisher = {Brains, Minds \& Media},
  year      = {2006},
  author    = {Cruse, H.},
  owner     = {fschubert},
  timestamp = {2017.01.30},
}

@Book{Dayan_2001,
  title     = {{Theoretical Neuroscience}},
  publisher = {MIT Press},
  year      = {2001},
  author    = {Dayan, P. and Abbott, L.F.},
  owner     = {fschubert},
  timestamp = {2016.04.12},
}

@Article{Desai_2003,
  author   = {Desai, N. S.},
  title    = {{Homeostatic plasticity in the CNS: synaptic and intrinsic forms}},
  journal  = {Journal of Physiology-Paris},
  year     = {2003},
  volume   = {97},
  number   = {4–6},
  pages    = {391 - 402},
  issn     = {0928-4257},
  note     = {Neuroscience and Computation},
  doi      = {http://dx.doi.org/10.1016/j.jphysparis.2004.01.005},
  keywords = {Experience-dependent plasticity},
  url      = {//www.sciencedirect.com/science/article/pii/S0928425704000166},
}

@Article{Desai_1999,
  author    = {Desai, N. S. and Rutherford, L. C. and Turrigiano, G. G.},
  title     = {{Plasticity in the intrinsic Excitability of Cortical Pyramidal Neurons}},
  journal   = {Nature Neuroscience},
  year      = {1999},
  volume    = {2},
  pages     = {515-520},
  owner     = {fschubert},
  timestamp = {2016.10.11},
}

@Article{Dieci_1995,
  author    = {Dieci, L. and Van Vleck, E. S.},
  title     = {{Computation of a few Lyapunov exponents for continuous and discrete dynamical systems}},
  journal   = {Applied Numerical Mathematics},
  year      = {1995},
  volume    = {17},
  pages     = {275--291},
  file      = {:/home/fschubert/papers/Dieci_Van_Vleck-Compuation_of_a_few_Lyapunov_exponents_for_continuous_and_discrete_dynamical_systems.pdf:PDF},
  owner     = {fschubert},
  timestamp = {2019.04.12},
}

@Article{Dragoi_2003,
  author  = {Dragoi, G. and Harris, K. D. and Buzsáki, G.},
  title   = {{Place Representation within Hippocampal Networks Is Modified by Long-Term Potentiation}},
  journal = {Neuron},
  year    = {2003},
  volume  = {39},
  number  = {5},
  pages   = {843 - 853},
  issn    = {0896-6273},
  doi     = {http://dx.doi.org/10.1016/S0896-6273(03)00465-3},
  url     = {http://www.sciencedirect.com/science/article/pii/S0896627303004653},
}

@Article{Duarte_2014,
  author  = {Duarte, R. and Series, P. and Morrison, A.},
  title   = {{Self-Organized Artificial Grammar Learning in Spiking Neural Networks}},
  journal = {Conference: 36th Annual Conference of the Cognitive Science Society},
  year    = {2014},
}

@Article{Echeveste_2015,
  author    = {Echeveste, Rodrigo and Eckmann, Samuel and Gros, Claudius},
  title     = {{The fisher information as a neural guiding principle for independent component analysis}},
  journal   = {Entropy},
  year      = {2015},
  volume    = {17},
  number    = {6},
  pages     = {3838-3856},
  owner     = {fschubert},
  timestamp = {2018.07.16},
}

@Article{Echeveste_2014,
  author    = {Echeveste, Rodrigo and Gros, Claudius},
  title     = {{Generating Functionals for Computational Intelligence: The Fisher Information as an Objective Function for Self-Limiting Hebbian Learning Rules}},
  journal   = {Frontiers in Robotics and AI},
  year      = {2014},
  volume    = {1},
  pages     = {1},
  issn      = {2296-9144},
  abstract  = {Generating functionals may guide the evolution ofa dynamical system and constitute a possible route for handling the complexity of neural networks asrelevant for computational intelligence. We propose and explore a new objective function which allows toobtain plasticity rules for the afferent synaptic weights. The adaption rules are Hebbian and self-limitingand result from the minimization of the the Fisher information with respect to the synaptic flux.We perform a series of simulations examining the behavior of the new learning rules in various circumstances. The vector of synaptic weights aligns with the principal direction of input activities, whenever one is present. A linear discrimination is performed when there are two or more principal directions; directions having bimodal firing-ratedistributions, being characterized by a negative excesskurtosis, are preferred. We find robust performance and full homeostaticadaption of the synaptic weights results as a by-productof the synaptic flux minimization. This self-limiting behaviorallows for stable online learning for arbitrary durations.The neuron acquires new information when the statistics ofinput activities is changed at a certain point of the simulation,showing however a distinct resilience to unlearn previously acquired knowledge. Learning is fast when starting with randomlydrawn synaptic weights and substantially slower when thesynaptic weights are already fully adapted.},
  doi       = {10.3389/frobt.2014.00001},
  owner     = {fschubert},
  timestamp = {2017.08.31},
  url       = {http://journal.frontiersin.org/article/10.3389/frobt.2014.00001},
}

@Article{Eckmann_2008,
  author    = {Eckmann, J.P. and Jacobi, S. and Marom, S. and Moses, E. and Zbinden, C.},
  title     = {{Leader neurons in population bursts of 2D living neural networks}},
  journal   = {New Journal of Physics},
  year      = {2008},
  volume    = {10},
  doi       = {10.1088/1367-2630/10/1/015011},
  owner     = {fschubert},
  timestamp = {2016.12.22},
}

@Article{Effenberger_2015,
  author    = {Effenberger, F. and Jost, J.},
  title     = {{Self-Organization in Balanced State Networks by STDP and Homeostatic Plasticity}},
  journal   = {PLOS Computational Biology},
  year      = {2015},
  owner     = {fschubert},
  timestamp = {2016.12.22},
}

@Article{Elman_1990,
  author   = {Elman, J. L.},
  title    = {{Finding Structure in Time}},
  journal  = {Cognitive Science},
  year     = {1990},
  volume   = {14},
  number   = {2},
  pages    = {179-211},
  abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
  doi      = {10.1207/s15516709cog1402\_1},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402_1},
  file     = {:/home/fschubert/papers/Elman-Finding_Structure_in_Time.pdf:PDF},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1},
}

@Article{Foeldiak_1990,
  author  = {F\"oldiak, P.},
  title   = {{Forming sparse representations by local anti-Hebbian learning}},
  journal = {Biological Cybernetics},
  year    = {1990},
}

@Article{Barrett_2015,
  author    = {{Feldman Barrett}, L. and Simmons, W. K.},
  title     = {{Interoceptive predictions in the brain}},
  journal   = {Nature Reviews Neuroscience},
  year      = {2015},
  volume    = {16},
  number    = {7},
  pages     = {419--429},
  month     = {may},
  doi       = {10.1038/nrn3950},
  publisher = {Springer Nature},
}

@Article{Feldman_2012,
  author    = {Feldman, D.},
  title     = {{The Spike-Timing Dependence of Plasticity}},
  journal   = {Neuron},
  year      = {2012},
  owner     = {fschubert},
  timestamp = {2016.12.22},
}

@Article{Fox_2017,
  author    = {Fox, K. and Stryker, M.},
  title     = {{Integrating Hebbian and homeostatic plasticity: introduction}},
  journal   = {PPhilosophical Transactions of the Royal Society: Biological Sciences},
  year      = {2017},
  volume    = {372},
  file      = {:/home/fschubert/papers/Integrating_Hebbian_and_homeostatic_plasticity_Fox.pdf:PDF},
  owner     = {fschubert},
  timestamp = {2017.11.01},
}

@Article{Friston_2010,
  author    = {Friston, K.},
  title     = {{The free-energy principle: a unified brain theory?}},
  journal   = {Nature Reviews Neuroscience},
  year      = {2010},
  doi       = {10.1038/nrn2787},
  file      = {:/home/fschubert/papers/Friston_The free-energy principle A unified brain theory.pdf:PDF},
  owner     = {fschubert},
  timestamp = {2017.09.28},
}

@Article{Friston_2008,
  author    = {Friston, Karl},
  title     = {{Hierarchical Models in the Brain}},
  journal   = {PLOS Computational Biology},
  year      = {2008},
  volume    = {4},
  number    = {11},
  pages     = {1-24},
  month     = {11},
  abstract  = {Author Summary Models are essential to make sense of scientific data, but they may also play a central role in how we assimilate sensory information. In this paper, we introduce a general model that generates or predicts diverse sorts of data. As such, it subsumes many common models used in data analysis and statistical testing. We show that this model can be fitted to data using a single and generic procedure, which means we can place a large array of data analysis procedures within the same unifying framework. Critically, we then show that the brain has, in principle, the machinery to implement this scheme. This suggests that the brain has the capacity to analyse sensory input using the most sophisticated algorithms currently employed by scientists and possibly models that are even more elaborate. The implications of this work are that we can understand the structure and function of the brain as an inference machine. Furthermore, we can ascribe various aspects of brain anatomy and physiology to specific computational quantities, which may help understand both normal brain function and how aberrant inferences result from pathological processes associated with psychiatric disorders.},
  doi       = {10.1371/journal.pcbi.1000211},
  file      = {:/home/fschubert/papers/Friston_Hierarchical_Models_in_the_Brain.pdf:PDF},
  publisher = {Public Library of Science},
  url       = {https://doi.org/10.1371/journal.pcbi.1000211},
}

@Article{Friston_2005,
  author    = {Friston, K.},
  title     = {{A theory of cortical responses}},
  journal   = {Philosophical Transactions of the Royal Society of London B: Biological Sciences},
  year      = {2005},
  volume    = {360},
  number    = {1456},
  pages     = {815--836},
  issn      = {0962-8436},
  abstract  = {This article concerns the nature of evoked brain responses and the principles underlying their generation. We start with the premise that the sensory brain has evolved to represent or infer the causes of changes in its sensory inputs. The problem of inference is well formulated in statistical terms. The statistical fundaments of inference may therefore afford important constraints on neuronal implementation. By formulating the original ideas of Helmholtz on perception, in terms of modern-day statistical theories, one arrives at a model of perceptual inference and learning that can explain a remarkable range of neurobiological facts.It turns out that the problems of inferring the causes of sensory input (perceptual inference) and learning the relationship between input and cause (perceptual learning) can be resolved using exactly the same principle. Specifically, both inference and learning rest on minimizing the brain{\textquoteright}s free energy, as defined in statistical physics. Furthermore, inference and learning can proceed in a biologically plausible fashion. Cortical responses can be seen as the brain{\textquoteright}s attempt to minimize the free energy induced by a stimulus and thereby encode the most likely cause of that stimulus. Similarly, learning emerges from changes in synaptic efficacy that minimize the free energy, averaged over all stimuli encountered. The underlying scheme rests on empirical Bayes and hierarchical models of how sensory input is caused. The use of hierarchical models enables the brain to construct prior expectations in a dynamic and context-sensitive fashion. This scheme provides a principled way to understand many aspects of cortical organization and responses. The aim of this article is to encompass many apparently unrelated anatomical, physiological and psychophysical attributes of the brain within a single theoretical perspective.In terms of cortical architectures, the theoretical treatment predicts that sensory cortex should be arranged hierarchically, that connections should be reciprocal and that forward and backward connections should show a functional asymmetry (forward connections are driving, whereas backward connections are both driving and modulatory). In terms of synaptic physiology, it predicts associative plasticity and, for dynamic models, spike-timing-dependent plasticity. In terms of electrophysiology, it accounts for classical and extra classical receptive field effects and long-latency or endogenous components of evoked cortical responses. It predicts the attenuation of responses encoding prediction error with perceptual learning and explains many phenomena such as repetition suppression, mismatch negativity (MMN) and the P300 in electroencephalography. In psychophysical terms, it accounts for the behavioural correlates of these physiological phenomena, for example, priming and global precedence. The final focus of this article is on perceptual learning as measured with the MMN and the implications for empirical studies of coupling among cortical areas using evoked sensory responses.},
  doi       = {10.1098/rstb.2005.1622},
  eprint    = {http://rstb.royalsocietypublishing.org/content/360/1456/815.full.pdf},
  publisher = {The Royal Society},
  url       = {http://rstb.royalsocietypublishing.org/content/360/1456/815},
}

@Article{Friston_2003,
  author    = {Friston, K.},
  title     = {{Learning and inference in the brain}},
  journal   = {Neural Networks},
  year      = {2003},
  volume    = {16},
  number    = {9},
  pages     = {1325--1352},
  month     = {nov},
  doi       = {10.1016/j.neunet.2003.06.005},
  publisher = {Elsevier {BV}},
}

@Article{Friston_2006,
  author    = {Friston, K. and Kilner, J. and Harrison, L.},
  title     = {{A free energy principle for the brain}},
  journal   = {Journal of Physiology-Paris},
  year      = {2006},
  volume    = {100},
  number    = {1-3},
  pages     = {70--87},
  month     = {jul},
  doi       = {10.1016/j.jphysparis.2006.10.001},
  publisher = {Elsevier {BV}},
}

@Article{Funahashi_1992,
  author  = {Funahashi, K.-I. and Nakamura, Y.},
  title   = {{Approximation of Dynamical Systems by Continuous TIme Recurrent Neural Networks}},
  journal = {Neural Networks},
  year    = {1992},
  volume  = {6},
  pages   = {801--806},
}

@Article{Gallicchio_2017,
  author    = {Claudio Gallicchio and Alessio Micheli},
  title     = {{Echo State Property of Deep Reservoir Computing Networks}},
  journal   = {Cognitive Computation},
  year      = {2017},
  volume    = {9},
  number    = {3},
  pages     = {337--350},
  month     = {may},
  doi       = {10.1007/s12559-017-9461-9},
  file      = {:/home/fschubert/papers/Gallicchio_Micheli-Echo_State_Property_of_Deep_Reservoir_Computing_Networks.pdf:PDF},
  publisher = {Springer Nature},
}

@Article{Gerstner_1995,
  author    = {Gerstner, W.},
  title     = {{Time structure of the activity in neural network models}},
  journal   = {Phys. Rev. E},
  year      = {1995},
  volume    = {51},
  pages     = {738--758},
  month     = {Jan},
  doi       = {10.1103/PhysRevE.51.738},
  issue     = {1},
  numpages  = {0},
  publisher = {American Physical Society},
  url       = {http://link.aps.org/doi/10.1103/PhysRevE.51.738},
}

@Book{Gerstner_2002,
  title     = {{Spiking Neuron Models. Single Neurons, Populations, Plasticity.}},
  publisher = {Cambridge University Press},
  year      = {2002},
  author    = {Gerstner, W. and Kistler, W. M.},
  owner     = {fschubert},
  timestamp = {2016.04.12},
}

@Article{Gilson_2011,
  author    = {Gilson, M. and Fukai, T.},
  title     = {{Stability versus Neuronal Specialization for STDP: Long-Tail Weight Distributions Solve the Dilemma}},
  journal   = {PLOS ONE},
  year      = {2011},
  volume    = {6},
  number    = {10},
  pages     = {1-18},
  month     = {10},
  abstract  = {Spike-timing-dependent plasticity (STDP) modifies the weight (or strength) of synaptic connections between neurons and is considered to be crucial for generating network structure. It has been observed in physiology that, in addition to spike timing, the weight update also depends on the current value of the weight. The functional implications of this feature are still largely unclear. Additive STDP gives rise to strong competition among synapses, but due to the absence of weight dependence, it requires hard boundaries to secure the stability of weight dynamics. Multiplicative STDP with linear weight dependence for depression ensures stability, but it lacks sufficiently strong competition required to obtain a clear synaptic specialization. A solution to this stability-versus-function dilemma can be found with an intermediate parametrization between additive and multiplicative STDP. Here we propose a novel solution to the dilemma, named log-STDP, whose key feature is a sublinear weight dependence for depression. Due to its specific weight dependence, this new model can produce significantly broad weight distributions with no hard upper bound, similar to those recently observed in experiments. Log-STDP induces graded competition between synapses, such that synapses receiving stronger input correlations are pushed further in the tail of (very) large weights. Strong weights are functionally important to enhance the neuronal response to synchronous spike volleys. Depending on the input configuration, multiple groups of correlated synaptic inputs exhibit either winner-share-all or winner-take-all behavior. When the configuration of input correlations changes, individual synapses quickly and robustly readapt to represent the new configuration. We also demonstrate the advantages of log-STDP for generating a stable structure of strong weights in a recurrently connected network. These properties of log-STDP are compared with those of previous models. Through long-tail weight distributions, log-STDP achieves both stable dynamics for and robust competition of synapses, which are crucial for spike-based information processing.},
  doi       = {10.1371/journal.pone.0025339},
  publisher = {Public Library of Science},
  url       = {http://dx.doi.org/10.1371%2Fjournal.pone.0025339},
}

@Article{Guerguiev_2017,
  author    = {Guerguiev, J. and Lillicrap, T. P. and Richards, B. A.},
  title     = {{Towards deep learning with segregated dendrites}},
  journal   = {{eLife}},
  year      = {2017},
  volume    = {6},
  month     = {dec},
  doi       = {10.7554/elife.22901},
  publisher = {{eLife} Sciences Publications, Ltd},
}

@Article{Gusarov_2005,
  author    = {Gusarov, I. and Nudler, E.},
  title     = {{NO-mediated Cytoprotection: Instant Adaptation to Oxidative Stress in Bacteria}},
  journal   = {Proceedings of the National Academy of Sciences of the United States of America},
  year      = {2005},
  volume    = {102},
  number    = {102},
  pages     = {13855-13860},
  owner     = {fschubert},
  timestamp = {2016.10.12},
}

@Article{Haga_2017,
  author    = {Haga, T. and Fukai, T.},
  title     = {{Dendritic processing of spontaneous neuronal sequences for one-shot learning}},
  journal   = {bioRxiv},
  year      = {2017},
  abstract  = {Spontaneous firing sequences colloquially called "preplay" are fundamental features of hippocampal network physiology. Preplay sequences have been hypothesized to participate in hippocampal learning and memory, but such functional roles and their potential cellular mechanisms remain unexplored. Here, we report a computational model based on the functional propagation of preplay sequences in the CA3 neuronal network. The model instantiates two synaptic pathways in CA3 neurons, one for proximal dendrite-somatic interactions to generate intrinsic preplay sequences and the other for distal dendritic processing of extrinsic sensory signals. The core dendritic computation is the maximization of matching between patterned activities in the two compartments through nonlinear spike generation. The model performs robust one-shot learning with long-term stability and independence that are modulated by the plasticity of dendrite-targeted inhibition. This model demonstrates that learning models combined with dendritic computations can enable preplay sequences to act as templates for rapid and stable memory formation.},
  doi       = {10.1101/165613},
  eprint    = {https://www.biorxiv.org/content/early/2017/09/04/165613.full.pdf},
  owner     = {fschubert},
  publisher = {Cold Spring Harbor Laboratory},
  timestamp = {2017.12.21},
  url       = {https://www.biorxiv.org/content/early/2017/09/04/165613},
}

@Article{Haider_2006,
  author    = {Haider, B. and Duque, A. and Hasenstaub, A. R. and McCormick, D. A.},
  title     = {{Neocortical Network Activity In Vivo Is Generated through a Dynamic Balance of Excitation and Inhibition}},
  journal   = {Journal of Neuroscience},
  year      = {2006},
  volume    = {26},
  number    = {17},
  pages     = {4535--4545},
  issn      = {0270-6474},
  abstract  = {The recurrent excitatory and inhibitory connections between and within layers of the cerebral cortex are fundamental to the operation of local cortical circuits. Models of cortical function often assume that recurrent excitation and inhibition are balanced, and we recently demonstrated that spontaneous network activity in vitro contains a precise balance of excitation and inhibition; however, the existence of a balance between excitation and inhibition in the intact and spontaneously active cerebral cortex has not been directly tested. We examined this hypothesis in the prefrontal cortex in vivo, during the slow (\&lt;1 Hz) oscillation in ketamine{\textendash}xylazine-anesthetized ferrets. We measured persistent network activity (Up states) with extracellular multiple unit and local field potential recording, while simultaneously recording synaptic currents in nearby cells. We determined the reversal potential and conductance change over time during Up states and found that the body of Up state activity exhibited a steady reversal potential (-37 mV on average) for hundreds of milliseconds, even during substantial (21 nS on average) changes in membrane conductance. Furthermore, we found that both the initial and final segments of the Up state were characterized by significantly more depolarized reversal potentials and concomitant increases in excitatory conductance, compared with the stable middle portions of Up states. This ongoing temporal evolution between excitation and inhibition, which exhibits remarkable proportionality within and across neurons in active local networks, may allow for rapid transitions between relatively stable network states, permitting the modulation of neuronal responsiveness in a behaviorally relevant manner.},
  doi       = {10.1523/JNEUROSCI.5297-05.2006},
  eprint    = {http://www.jneurosci.org/content/26/17/4535.full.pdf},
  publisher = {Society for Neuroscience},
  url       = {http://www.jneurosci.org/content/26/17/4535},
}

@Article{Hall_2009,
  author   = {Hall, C. N. and Garthwaite, J.},
  title    = {{What is the real physiological \{NO\} concentration in vivo?}},
  journal  = {Nitric Oxide},
  year     = {2009},
  volume   = {21},
  number   = {2},
  pages    = {92 - 103},
  issn     = {1089-8603},
  doi      = {http://dx.doi.org/10.1016/j.niox.2009.07.002},
  keywords = {Nitric oxide},
  url      = {http://www.sciencedirect.com/science/article/pii/S1089860309000962},
}

@Article{Harnack_2015,
  author    = {Harnack, D. and Pelko, M. and Chaillet, A. and Chitour, Y. and van Rossum, M. C. W.},
  title     = {{Stability of Neuronal Networks with Homeostatic Regulation}},
  journal   = {PLOS Computational Biology},
  year      = {2015},
  owner     = {fschubert},
  timestamp = {2017.01.17},
}

@Article{Hartmann_2016,
  author    = {Hartmann, C. and Lazar, A. and Nessler, B. and Triesch, J.},
  title     = {{Where’s the Noise? Key Features of Spontaneous Activity and Neural Variability Arise through Learning in a Deterministic Network}},
  journal   = {PLOS Computational Biology},
  year      = {2016},
  volume    = {11},
  number    = {12},
  pages     = {1-35},
  month     = {12},
  abstract  = {Author Summary Neural recordings seem very noisy. If the exact same stimulus is shown to an animal multiple times, the neural response will vary substantially. In fact, the activity of a single neuron shows many features of a random process. Furthermore, the spontaneous activity occurring in the absence of any sensory stimulus, which is usually considered a kind of background noise, often has a magnitude comparable to the activity evoked by stimulus presentation and interacts with sensory inputs in interesting ways. Here we show that the key features of neural variability and spontaneous activity can all be accounted for by a simple and completely deterministic neural network learning a predictive model of its sensory inputs. The network’s deterministic dynamics give rise to structured but variable responses matching key experimental findings obtained in different mammalian species with different recording techniques. Our results suggest that the notorious variability of neural recordings and the complex features of spontaneous brain activity could reflect the dynamics of a largely deterministic but highly adaptive network learning a predictive model of its sensory environment.},
  doi       = {10.1371/journal.pcbi.1004640},
  publisher = {Public Library of Science},
  url       = {http://dx.doi.org/10.1371%2Fjournal.pcbi.1004640},
}

@Article{Hawkins_2016,
  author    = {Hawkins, J. and Ahmad, S.},
  title     = {{Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in Neocortex}},
  journal   = {Frontiers in Neural Circuits},
  year      = {2016},
  volume    = {10},
  month     = {mar},
  doi       = {10.3389/fncir.2016.00023},
  publisher = {Frontiers Media {SA}},
}

@Article{Hengen_2016,
  author    = {Hengen, K. B. and Torrado Pacheco, A. and McGregor, J. N. and Van Hooser, S. D. and Turrigiano, G. G.},
  title     = {{Neuronal Firing Rate Homeostasis Is Inhibited by Sleep and Promoted by Wake}},
  journal   = {Cell},
  year      = {2016},
  volume    = {165},
  number    = {1},
  pages     = {180--191},
  issn      = {0092-8674},
  abstract  = {Homeostatic mechanisms stabilize neural circuit function by keeping firing rates within a set-point range, but whether this process is gated by brain state is unknown. Here, we monitored firing rate homeostasis in individual visual cortical neurons in freely behaving rats as they cycled between sleep and wake states. When neuronal firing rates were perturbed by visual deprivation, they gradually returned to a precise, cell-autonomous set point during periods of active wake, with lengthening of the wake period enhancing firing rate rebound. Unexpectedly, this resetting of neuronal firing was suppressed during sleep. This raises the possibility that memory consolidation or other sleep-dependent processes are vulnerable to interference from homeostatic plasticity mechanisms.
Homeostatic mechanisms stabilize neural circuit function by keeping firing rates within a set-point range, but whether this process is gated by brain state is unknown. Here, we monitored firing rate homeostasis in individual visual cortical neurons in freely behaving rats as they cycled between sleep and wake states. When neuronal firing rates were perturbed by visual deprivation, they gradually returned to a precise, cell-autonomous set point during periods of active wake, with lengthening of the wake period enhancing firing rate rebound. Unexpectedly, this resetting of neuronal firing was suppressed during sleep. This raises the possibility that memory consolidation or other sleep-dependent processes are vulnerable to interference from homeostatic plasticity mechanisms.},
  booktitle = {Cell},
  comment   = {doi: 10.1016/j.cell.2016.01.046},
  doi       = {10.1016/j.cell.2016.01.046},
  publisher = {Elsevier},
  url       = {http://dx.doi.org/10.1016/j.cell.2016.01.046},
}

@Article{Hill_Equ,
  author  = {Hill, AV.},
  title   = {{The possible effects of the aggregation of the molecules of haemoglobin on its dissociation curves}},
  journal = {The Journal of Physiology},
  year    = {1910},
  volume  = {40},
  pages   = {iv---vii},
  issn    = {1469-7793},
  doi     = {10.1113/jphysiol.1910.sp001389},
  url     = {http://dx.doi.org/10.1113/jphysiol.1910.sp001389},
}

@Book{Hille_2001,
  title     = {{Ionic Channels of Excitable Membranes}},
  publisher = {Sinauer Associates},
  year      = {2001},
  author    = {Hille, B.},
  owner     = {fschubert},
  timestamp = {2016.10.12},
}

@Article{Hodgkin_Huxley_1952,
  author  = {Hodgkin, A. L. and Huxley, A. F.},
  title   = {{A quantitative description of membrane current and its application to conduction and excitation in nerve}},
  journal = {The Journal of Physiology},
  year    = {1952},
  volume  = {117},
  number  = {4},
  pages   = {500--544},
  month   = aug,
  issn    = {1469-7793},
  comment = {12991237[pmid] 12991237[pmid]},
  url     = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413/},
}

@Article{Hoffmann_2017,
  author    = {Hoffmann, F. Z. and Triesch, J.},
  title     = {{Non-random network connectivity comes in pairs}},
  journal   = {Network Neuroscience},
  year      = {2017},
  owner     = {fabian},
  timestamp = {2017.02.20},
}

@Article{Ivenshitz_2010,
  author    = {Ivenshitz, M. and Segal, M.},
  title     = {{Neuronal Density Determines Network Connectivity and Spontaneous Activity in Cultured Hippocampus}},
  journal   = {Journal of Neurophysiology},
  year      = {2010},
  volume    = {104},
  number    = {2},
  pages     = {1052--1060},
  issn      = {0022-3077},
  abstract  = {The effects of neuronal density on morphological and functional attributes of the evolving networks were studied in cultured dissociated hippocampal neurons. Plating at different densities affected connectivity among the neurons, such that sparse networks exhibited stronger synaptic connections between pairs of recorded neurons. This was associated with different patterns of spontaneous network activity with enhanced burst size but reduced burst frequency in the sparse cultures. Neuronal density also affected the morphology of the dendrites and spines of these neurons, such that sparse neurons had a simpler dendritic tree and fewer dendritic spines. Additionally, analysis of neurons transfected with PSD95 revealed that in sparse cultures the synapses are formed on the dendritic shaft, whereas in dense cultures the synapses are formed primarily on spine heads. These experiments provide important clues on the role of neuronal density in population activity and should yield new insights into the rules governing neuronal network connectivity.},
  doi       = {10.1152/jn.00914.2009},
  eprint    = {http://jn.physiology.org/content/104/2/1052.full.pdf},
  publisher = {American Physiological Society},
  url       = {http://jn.physiology.org/content/104/2/1052},
}

@Book{Izhikevich_Dynsys,
  title     = {{Dynamical Systems in Neuroscience - The Geometry of Excitability and Bursting}},
  publisher = {The MIT Press},
  year      = {2007},
  author    = {Izhikevich, E. M.},
  owner     = {fschubert},
  timestamp = {2016.10.11},
}

@TechReport{Jaeger_2002,
  author      = {Jaeger, H.},
  title       = {{Short Term Memory in Echo State Networks}},
  institution = {Fraunhofer Institute for Autonomous Intelligent Systems},
  year        = {2002},
  type        = {GMD Report},
  number      = {152},
  owner       = {fschubert},
  timestamp   = {2019.04.17},
}

@TechReport{Jaeger_2001,
  author      = {Jaeger, H.},
  title       = {{The "echo state" approach to analysing and training recurrent neural networks}},
  institution = {GMD - German National Research Institute for Computer Science},
  year        = {2001},
  type        = {GMD Report},
  number      = {148},
  added-at    = {2008-03-11T14:52:34.000+0100},
  biburl      = {https://www.bibsonomy.org/bibtex/23d434b04cf1479acf45be9af65f8bc78/idsia},
  interhash   = {c24261c15e88abe24a39d13339cc7697},
  intrahash   = {3d434b04cf1479acf45be9af65f8bc78},
  keywords    = {imported},
  timestamp   = {2008-03-11T14:52:36.000+0100},
  url         = {http://www.faculty.jacobs-university.de/hjaeger/pubs/EchoStatesTechRep.pdf},
}

@Article{Jolivet_2006,
  author    = {Jolivet, R. and Rauch, A. and L{\"u}scher, H.-R. and Gerstner, W.},
  title     = {{Predicting spike timing of neocortical pyramidal neurons by simple threshold models}},
  journal   = {Journal of Computational Neuroscience},
  year      = {2006},
  volume    = {21},
  number    = {1},
  pages     = {35--49},
  issn      = {1573-6873},
  abstract  = {Neurons generate spikes reliably with millisecond precision if driven by a fluctuating current---is it then possible to predict the spike timing knowing the input? We determined parameters of an adapting threshold model using data recorded in vitro from 24 layer 5 pyramidal neurons from rat somatosensory cortex, stimulated intracellularly by a fluctuating current simulating synaptic bombardment in vivo. The model generates output spikes whenever the membrane voltage (a filtered version of the input current) reaches a dynamic threshold. We find that for input currents with large fluctuation amplitude, up to 75{\%} of the spike times can be predicted with a precision of {\textpm}2 ms. Some of the intrinsic neuronal unreliability can be accounted for by a noisy threshold mechanism. Our results suggest that, under random current injection into the soma, (i) neuronal behavior in the subthreshold regime can be well approximated by a simple linear filter; and (ii) most of the nonlinearities are captured by a simple threshold process.},
  doi       = {10.1007/s10827-006-7074-5},
  owner     = {fabian},
  timestamp = {2017.06.20},
  url       = {http://dx.doi.org/10.1007/s10827-006-7074-5},
}

@InCollection{Jordan_1989,
  author               = {Jordan, Michael I.},
  title                = {{Serial Order: {A} Parallel, Distributed Processing Approach}},
  booktitle            = {Advances in Connectionist Theory: {S}peech},
  publisher            = {Erlbaum},
  year                 = {1989},
  editor               = {Elman, Jeffrey L. and Rumelhart, David E.},
  address              = {Hillsdale, NJ},
  added-at             = {2008-02-26T11:58:58.000+0100},
  biburl               = {https://www.bibsonomy.org/bibtex/2e5eda61328721c6b868b1a7185c2dd14/schaul},
  citeulike-article-id = {2377338},
  description          = {idsia},
  interhash            = {eefdf3e7f975f49d62ec84818d5e846b},
  intrahash            = {e5eda61328721c6b868b1a7185c2dd14},
  keywords             = {nn},
  timestamp            = {2008-02-26T12:04:26.000+0100},
}

@Article{Kay_2010,
  author    = {Kay, J. W. and Phillips, W. A.},
  title     = {{Coherent Infomax as a Computational Goal for~Neural~Systems}},
  journal   = {Bulletin of Mathematical Biology},
  year      = {2010},
  volume    = {73},
  number    = {2},
  pages     = {344--372},
  month     = {sep},
  doi       = {10.1007/s11538-010-9564-x},
  publisher = {Springer Nature},
}

@Article{Kirst_2016,
  author    = {Kirst, C. and Timme, M. and Battaglia, D.},
  title     = {{Dynamic information routing in complex networks}},
  journal   = {Nature Communications},
  year      = {2016},
  volume    = {7},
  pages     = {11061},
  month     = {apr},
  doi       = {10.1038/ncomms11061},
  publisher = {Springer Nature},
}

@Article{Knowles_1994,
  author    = {Knowles, R. G. and Moncada, S.},
  title     = {{Nitric Oxide Synthases in Mammals}},
  journal   = {Biochemical Journal},
  year      = {1994},
  volume    = {298(Pt 2)},
  pages     = {249-258},
  owner     = {fschubert},
  timestamp = {2016.10.12},
}

@Article{Knowles_1989,
  author    = {Knowles, R. G. and Palacios, M. and Palmer, R. M. J. and Moncada, S.},
  title     = {{Formation of nitric oxide from L-arginine in the central nervous system: a transduction mechanism for stimulation of the soluble guanylate cyclase}},
  journal   = {Proceedings of the National Academy of Sciences},
  year      = {1989},
  volume    = {86},
  pages     = {5159-5162},
  owner     = {fschubert},
  timestamp = {2016.10.12},
}

@Article{Koulakov_2009,
  author    = {Koulakov, A. A. and Hrom\'{a}dka, T. and Zador, A. M.},
  title     = {{Correlated Connectivity and the Distribution of Firing Rates in the Neocortex}},
  journal   = {The Journal of Neuroscience},
  year      = {2009},
  owner     = {fschubert},
  timestamp = {2017.02.22},
}

@Article{Luetcke_2011,
  author  = {L\"utcke, H. and Helmchen, F.},
  title   = {{Two-photon imaging and analysis of neural network dynamics}},
  journal = {Reports on Progress in Physics},
  year    = {2011},
  volume  = {74},
  number  = {8},
  pages   = {086602},
  url     = {http://stacks.iop.org/0034-4885/74/i=8/a=086602},
}

@Article{Lancaster_1994,
  author   = {Lancaster, J. R.},
  title    = {{Simulation of the diffusion and reaction of endogenously produced nitric oxide}},
  journal  = {Proceedings of the National Academy of Sciences of the United States of America},
  year     = {1994},
  volume   = {91},
  number   = {17},
  pages    = {8137--8141},
  month    = aug,
  issn     = {1091-6490},
  abstract = {In spite of intense recent investigation of the physiological and pathophysiological roles of endogenously produced nitric oxide (.NO) in mammalian systems, little quantitative information exists concerning the diffusion of this small nonelectrolyte from its source (NO synthase) to its targets of action. I present here a conceptual framework for analyzing the intracellular and intercellular diffusion and reaction of free .NO, using kinetic modeling and calculations of the diffusibility of .NO and its reactions in aqueous solution based on published data. If the half-life of .NO is greater than approximately 25 msec and the rates of reaction of .NO with its targets are slower than its diffusion or reaction with O2 (for which there is experimental evidence in at least some systems), then (i) .NO acts in vivo in a mostly paracrine fashion for a collection of .NO-producing cells, (ii) .NO diffuses to significant concentrations at distances relatively far removed from a single .NO-producing cell, and (iii) localized sites of vascularization will scavenge .NO (and thus decrease its actions) at distances many cell diameters away from that site. These conclusions have important implications with regard to the mechanism of endothelium-dependent relaxation, the autocrine vs. paracrine actions of .NO, and the role of the spatial relationship between specific sites of .NO formation and neighboring blood vessels in .NO-effected and -affected neuronal signal transmission.},
  comment  = {8058769[pmid] 8058769[pmid]},
  url      = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC44560/},
}

@Article{Larkum_1999,
  author  = {Larkum, M. E. and Zhu, J. J. and Sakmann, B.},
  title   = {{A new cellular mechanism for coupling inputs arriving at different cortical layers}},
  journal = {Nature},
  year    = {1999},
  volume  = {398},
  pages   = {338--341},
}

@Article{Lazar_2009,
  author    = {Lazar, A. and Pipa, G. and Triesch, J.},
  title     = {{SORN: a self-organizing recurrent neural network}},
  journal   = {Frontiers in Computational Neuroscience},
  year      = {2009},
  volume    = {3},
  owner     = {fschubert},
  timestamp = {2017.02.16},
}

@Article{Lefort_2009,
  author    = {Lefort, S. and Tomm, C. and Floyd Sarria, J.-C. and Petersen, Carl C.H.},
  title     = {{The Excitatory Neuronal Network of the C2 Barrel Column in Mouse Primary Somatosensory Cortex}},
  journal   = {Neuron},
  year      = {2009},
  volume    = {61},
  number    = {2},
  pages     = {301--316},
  issn      = {0896-6273},
  booktitle = {Neuron},
  comment   = {doi: 10.1016/j.neuron.2008.12.020},
  doi       = {10.1016/j.neuron.2008.12.020},
  keywords  = {log-normal},
  owner     = {fabian},
  publisher = {Elsevier},
  timestamp = {2017.05.09},
  url       = {http://dx.doi.org/10.1016/j.neuron.2008.12.020},
}

@Article{LeMasson_1993,
  author    = {LeMasson, G. and Marder, E. and Abbott, L. F.},
  title     = {{Activity-dependent regulation of conductances in model neurons}},
  journal   = {Science},
  year      = {1993},
  volume    = {259},
  number    = {5103},
  pages     = {1915--1917},
  issn      = {0036-8075},
  abstract  = {Neurons maintain their electrical activity patterns despite channel turnover, cell growth, and variable extracellular conditions. A model is presented in which maximal conductances of ionic currents depend on the intracellular concentration of calcium ions and so, indirectly, on activity. Model neurons with activity-dependent maximal conductances modify their conductances to maintain a given behavior when perturbed. Moreover, neurons that are described by identical sets of equations can develop different properties in response to different patterns of presynaptic activity.},
  doi       = {10.1126/science.8456317},
  eprint    = {http://science.sciencemag.org/content/259/5103/1915.full.pdf},
  publisher = {American Association for the Advancement of Science},
  url       = {http://science.sciencemag.org/content/259/5103/1915},
}

@Article{Letzkus_2006,
  author    = {Letzkus, Johannes J. and Kampa, Bj{\"o}rn M. and Stuart, Greg J.},
  title     = {{Learning Rules for Spike Timing-Dependent Plasticity Depend on Dendritic Synapse Location}},
  journal   = {Journal of Neuroscience},
  year      = {2006},
  volume    = {26},
  number    = {41},
  pages     = {10420--10429},
  issn      = {0270-6474},
  abstract  = {Previous studies focusing on the temporal rules governing changes in synaptic strength during spike timing-dependent synaptic plasticity (STDP) have paid little attention to the fact that synaptic inputs are distributed across complex dendritic trees. During STDP, propagation of action potentials (APs) back to the site of synaptic input is thought to trigger plasticity. However, in pyramidal neurons, backpropagation of single APs is decremental, whereas high-frequency bursts lead to generation of distal dendritic calcium spikes. This raises the question whether STDP learning rules depend on synapse location and firing mode. Here, we investigate this issue at synapses between layer 2/3 and layer 5 pyramidal neurons in somatosensory cortex. We find that low-frequency pairing of single APs at positive times leads to a distance-dependent shift to long-term depression (LTD) at distal inputs. At proximal sites, this LTD could be converted to long-term potentiation (LTP) by dendritic depolarizations suprathreshold for BAC-firing or by high-frequency AP bursts. During AP bursts, we observed a progressive, distance-dependent shift in the timing requirements for induction of LTP and LTD, such that distal synapses display novel timing rules: they potentiate when inputs are activated after burst onset (negative timing) but depress when activated before burst onset (positive timing). These findings could be explained by distance-dependent differences in the underlying dendritic voltage waveforms driving NMDA receptor activation during STDP induction. Our results suggest that synapse location within the dendritic tree is a crucial determinant of STDP, and that synapses undergo plasticity according to local rather than global learning rules.},
  doi       = {10.1523/JNEUROSCI.2650-06.2006},
  eprint    = {http://www.jneurosci.org/content/26/41/10420.full.pdf},
  owner     = {fschubert},
  publisher = {Society for Neuroscience},
  timestamp = {2018.07.20},
  url       = {http://www.jneurosci.org/content/26/41/10420},
}

@Article{Lever_2002,
  author    = {Lever, C. and Wills, T. and Cacucci, F. and Burgess, N. and O'Keefe, J.},
  title     = {{Long-term plasticity in hippocampal place-cell representation of environmental geometry}},
  journal   = {Nature},
  year      = {2002},
  owner     = {fschubert},
  timestamp = {2017.01.17},
}

@Article{Lisman_1993,
  author  = {Lisman, J. E. and Harris, K. M.},
  title   = {{Quantal analysis and synaptic anatomy — integrating two views of hippocampal plasticity}},
  journal = {Trends in Neurosciences},
  year    = {1993},
  volume  = {16},
  number  = {4},
  pages   = {141 - 147},
  issn    = {0166-2236},
  doi     = {http://dx.doi.org/10.1016/0166-2236(93)90122-3},
  url     = {http://www.sciencedirect.com/science/article/pii/0166223693901223},
}

@Article{Livi_2016,
  author    = {Livi, L. and Bianchi, F. M. and Alippi, C.},
  title     = {{Determination of the edge of criticality in echo state networks through Fisher information maximization}},
  journal   = {arXiv:1603.03685v2},
  year      = {2016},
  owner     = {fschubert},
  timestamp = {2019.02.12},
}

@Article{Loewenstein_2011,
  author    = {Loewenstein, Y. and Kuras, A. and Rumpel, S.},
  title     = {{Multiplicative Dynamics Underlie the Emergence of the Log-Normal Distribution of Spine Sizes in the Neocortex In Vivo}},
  journal   = {Journal of Neuroscience},
  year      = {2011},
  volume    = {31},
  number    = {26},
  pages     = {9481--9488},
  issn      = {0270-6474},
  abstract  = {What fundamental properties of synaptic connectivity in the neocortex stem from the ongoing dynamics of synaptic changes? In this study, we seek to find the rules shaping the stationary distribution of synaptic efficacies in the cortex. To address this question, we combined chronic imaging of hundreds of spines in the auditory cortex of mice in vivo over weeks with modeling techniques to quantitatively study the dynamics of spines, the morphological correlates of excitatory synapses in the neocortex. We found that the stationary distribution of spine sizes of individual neurons can be exceptionally well described by a log-normal function. We furthermore show that spines exhibit substantial volatility in their sizes at timescales that range from days to months. Interestingly, the magnitude of changes in spine sizes is proportional to the size of the spine. Such multiplicative dynamics are in contrast with conventional models of synaptic plasticity, learning, and memory, which typically assume additive dynamics. Moreover, we show that the ongoing dynamics of spine sizes can be captured by a simple phenomenological model that operates at two timescales of days and months. This model converges to a log-normal distribution, bridging the gap between synaptic dynamics and the stationary distribution of synaptic efficacies.},
  doi       = {10.1523/JNEUROSCI.6130-10.2011},
  eprint    = {http://www.jneurosci.org/content/31/26/9481.full.pdf},
  publisher = {Society for Neuroscience},
  url       = {http://www.jneurosci.org/content/31/26/9481},
}

@Article{Loewenstein_2015,
  author    = {Loewenstein, Y. and Yanover, U. and Rumpel, S.},
  title     = {{Predicting the Dynamics of Network Connectivity in the Neocortex}},
  journal   = {Journal of Neuroscience},
  year      = {2015},
  volume    = {35},
  number    = {36},
  pages     = {12535--12544},
  issn      = {0270-6474},
  abstract  = {Dynamic remodeling of connectivity is a fundamental feature of neocortical circuits. Unraveling the principles underlying these dynamics is essential for the understanding of how neuronal circuits give rise to computations. Moreover, as complete descriptions of the wiring diagram in cortical tissues are becoming available, deciphering the dynamic elements in these diagrams is crucial for relating them to cortical function. Here, we used chronic in vivo two-photon imaging to longitudinally follow a few thousand dendritic spines in the mouse auditory cortex to study the determinants of these spines{\textquoteright} lifetimes. We applied nonlinear regression to quantify the independent contribution of spine age and several morphological parameters to the prediction of the future survival of a spine. We show that spine age, size, and geometry are parameters that can provide independent contributions to the prediction of the longevity of a synaptic connection. In addition, we use this framework to emulate a serial sectioning electron microscopy experiment and demonstrate how incorporation of morphological information of dendritic spines from a single time-point allows estimation of future connectivity states. The distinction between predictable and nonpredictable connectivity changes may be used in the future to identify the specific adaptations of neuronal circuits to environmental changes. The full dataset is publicly available for further analysis.SIGNIFICANCE STATEMENT The neural architecture in the neocortex exhibits constant remodeling. The functional consequences of these modifications are poorly understood, in particular because the determinants of these changes are largely unknown. Here, we aimed to identify those modifications that are predictable from current network state. To that goal, we repeatedly imaged thousands of dendritic spines in the auditory cortex of mice to assess the morphology and lifetimes of synaptic connections. We developed models based on morphological features of dendritic spines that allow predicting future turnover of synaptic connections. The dynamic models presented in this paper provide a quantitative framework for adding putative temporal dynamics to the static description of a neuronal circuit from single time-point connectomics experiments.},
  doi       = {10.1523/JNEUROSCI.2917-14.2015},
  eprint    = {http://www.jneurosci.org/content/35/36/12535.full.pdf},
  owner     = {fabian},
  publisher = {Society for Neuroscience},
  timestamp = {2017.05.21},
  url       = {http://www.jneurosci.org/content/35/36/12535},
}

@Article{Lukosevicius_2009,
  author    = {Mantas Lukoševičius and Herbert Jaeger},
  title     = {{Reservoir computing approaches to recurrent neural network training}},
  journal   = {Computer Science Review},
  year      = {2009},
  volume    = {3},
  number    = {3},
  pages     = {127 - 149},
  issn      = {1574-0137},
  abstract  = {Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current “brand-names” of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed “map” of it.},
  doi       = {https://doi.org/10.1016/j.cosrev.2009.03.005},
  file      = {:/home/fschubert/papers/Lukosevicius_Jaeger-Reservoir_Computing_Approaches_to_Recurrent_Neural_Network_Training.pdf:PDF},
  owner     = {fschubert},
  timestamp = {2018.07.20},
  url       = {http://www.sciencedirect.com/science/article/pii/S1574013709000173},
}

@Article{Luz_2012,
  author    = {Luz, Y. AND Shamir, M.},
  title     = {{Balancing Feed-Forward Excitation and Inhibition via Hebbian Inhibitory Synaptic Plasticity}},
  journal   = {PLOS Computational Biology},
  year      = {2012},
  volume    = {8},
  number    = {1},
  pages     = {1-12},
  month     = {01},
  abstract  = {Author Summary One of the longstanding enigmas in neuroscience is the origin of inherent neural noise. It has been suggested that this noise results from a careful balance of excitatory and inhibitory inputs to neurons. Obtaining this balance requires fine tuning of the relative strengths of the inhibitory and excitatory inputs to each cell. However the mechanism that enables this fine tuning of parameters remains unclear. We suggest that a balance of excitatory and inhibitory inputs can be achieved via a process of unsupervised learning of the inhibitory synaptic strengths. We find that whereas Hebbian learning induces strong positive feedback on excitatory inputs that acts as a force that pulls them towards their boundaries, Hebbian learning of inhibition induces negative feedback. This negative feedback acts to balance the average excitatory input to the cell and makes the learning process less sensitive to the statistics of the inhibitory inputs. Surprisingly, this balance increases the sensitivity of learning to the statistics of the excitatory inputs. Thus, the balance of feed-forward excitation and inhibition emerges as a natural outcome of Hebbian learning applied to the inhibitory inputs to the cell.},
  doi       = {10.1371/journal.pcbi.1002334},
  publisher = {Public Library of Science},
  url       = {http://dx.doi.org/10.1371%2Fjournal.pcbi.1002334},
}

@Article{Manita_2017,
  author    = {Manita, S. and Miyakawa, H. and Kitamura, K. and Murayama, M.},
  title     = {{Dendritic Spikes in Sensory Perception}},
  journal   = {Frontiers in Cellular Neuroscience},
  year      = {2017},
  volume    = {11},
  month     = {feb},
  doi       = {10.3389/fncel.2017.00029},
  publisher = {Frontiers Media {SA}},
}

@Article{Manjunath_2013,
  author   = {Manjunath, G. and Jaeger, H.},
  title    = {{Echo State Property Linked to an Input: Exploring a Fundamental Characteristic of Recurrent Neural Networks}},
  journal  = {Neural Computation},
  year     = {2013},
  volume   = {25},
  number   = {3},
  pages    = {671-696},
  note     = {PMID: 23272918},
  abstract = { The echo state property is a key for the design and training of recurrent neural networks within the paradigm of reservoir computing. In intuitive terms, this is a passivity condition: a network having this property, when driven by an input signal, will become entrained by the input and develop an internal response signal. This excited internal dynamics can be seen as a high-dimensional, nonlinear, unique transform of the input with a rich memory content. This view has implications for understanding neural dynamics beyond the field of reservoir computing. Available definitions and theorems concerning the echo state property, however, are of little practical use because they do not relate the network response to temporal or statistical properties of the driving input. Here we present a new definition of the echo state property that directly connects it to such properties. We derive a fundamental 0-1 law: if the input comes from an ergodic source, the network response has the echo state property with probability one or zero, independent of the given network. Furthermore, we give a sufficient condition for the echo state property that connects statistical characteristics of the input to algebraic properties of the network connection matrix. The mathematical methods that we employ are freshly imported from the young field of nonautonomous dynamical systems theory. Since these methods are not yet well known in neural computation research, we introduce them in some detail. As a side story, we hope to demonstrate the eminent usefulness of these methods. },
  doi      = {10.1162/NECO\_a\_00411},
  eprint   = {https://doi.org/10.1162/NECO_a_00411},
  file     = {:/home/fschubert/papers/Manjunath_Jaeger-Echo_State_Property_Linked_to_an_Input_Exploring_a_Fundamental_Characteristic_of_Recurrent_Neural_Networks.pdf:PDF},
  url      = { 
 https://doi.org/10.1162/NECO_a_00411
 
},
}

@Article{Marder_2006,
  author    = {Marder, E. and Goaillard, J.},
  title     = {{Variability, compensation and homeostasis in neuron and network function}},
  journal   = {Nature Reviews Neuroscience},
  year      = {2006},
  owner     = {fabian},
  timestamp = {2017.01.29},
}

@Article{Markram_1997,
  author   = {Markram, H.},
  title    = {{A network of tufted layer 5 pyramidal neurons.}},
  journal  = {Cerebral Cortex},
  year     = {1997},
  volume   = {7},
  number   = {6},
  pages    = {523-533},
  abstract = {Tufted layer 5 (TL5) pyramidal neurons are important projection neurons from the cerebral cortex to subcortical areas. Recent and ongoing experiments aimed at understanding the computational analysis performed by a network of synaptically connected TL5 neurons are reviewed here. The experiments employed dual and triple whole-cell patch clamp recordings from visually identified and preselected neurons in brain slices of somatosensory cortex of young (14- to 16-day-old) rats. These studies suggest that a local network of TL5 neurons within a cortical module of diameter 300 microns consists of a few hundred neurons that are extensively inter-connected with reciprocal feedback from at least first-, second- and third-order target neurons. A statistical analysis of synaptic innervation suggests that this recurrent network is not randomly arranged and hence each neuron could be functionally unique. Synaptic transmission between these neurons is characterized by use-dependent synaptic depression which confers novel properties to this recurrent network of neurons. First, a range of rates of depression for different synaptic connections enable each TL5 neuron to receive a unique mixture of information about the average firing rates and the temporally correlated action potential (AP) activity in the population of presynaptic TL5 neurons. Second, each AP generated by any neuron in the network induces a change (defined as an iteration step) in the functional coupling of the neurons in the network (defined as network configuration). It is proposed that the network configuration is iterated during a stimulus to achieve an optimally orchestrated network response. Hebbian, anti-Hebbian and neuromodulatory-induced modifications of neurotransmitter release probability change the rates of synaptic depression and thereby alter the iteration step size. These data may be important to understand the dynamics of electrical activity within the network.},
  doi      = {10.1093/cercor/7.6.523},
  eprint   = {http://cercor.oxfordjournals.org/content/7/6/523.full.pdf+html},
  url      = {http://cercor.oxfordjournals.org/content/7/6/523.abstract},
}

@Article{Markram_1998,
  author   = {Markram, H. and Wang, Y. and Tsodyks, M.},
  title    = {{Differential signaling via the same axon of neocortical pyramidal neurons}},
  journal  = {Proceedings of the National Academy of Sciences},
  year     = {1998},
  volume   = {95},
  number   = {9},
  pages    = {5323-5328},
  abstract = {The nature of information stemming from a single neuron and conveyed simultaneously to several hundred target neurons is not known. Triple and quadruple neuron recordings revealed that each synaptic connection established by neocortical pyramidal neurons is potentially unique. Specifically, synaptic connections onto the same morphological class differed in the numbers and dendritic locations of synaptic contacts, their absolute synaptic strengths, as well as their rates of synaptic depression and recovery from depression. The same axon of a pyramidal neuron innervating another pyramidal neuron and an interneuron mediated frequency-dependent depression and facilitation, respectively, during high frequency discharges of presynaptic action potentials, suggesting that the different natures of the target neurons underlie qualitative differences in synaptic properties. Facilitating-type synaptic connections established by three pyramidal neurons of the same class onto a single interneuron, were all qualitatively similar with a combination of facilitation and depression mechanisms. The time courses of facilitation and depression, however, differed for these convergent connections, suggesting that different pre-postsynaptic interactions underlie quantitative differences in synaptic properties. Mathematical analysis of the transfer functions of frequency-dependent synapses revealed supra-linear, linear, and sub-linear signaling regimes in which mixtures of presynaptic rates, integrals of rates, and derivatives of rates are transferred to targets depending on the precise values of the synaptic parameters and the history of presynaptic action potential activity. Heterogeneity of synaptic transfer functions therefore allows multiple synaptic representations of the same presynaptic action potential train and suggests that these synaptic representations are regulated in a complex manner. It is therefore proposed that differential signaling is a key mechanism in neocortical information processing, which can be regulated by selective synaptic modifications.},
  eprint   = {http://www.pnas.org/content/95/9/5323.full.pdf},
  url      = {http://www.pnas.org/content/95/9/5323.abstract},
}

@Article{Marsat_2010,
  author    = {Marsat, G. and Maler, L.},
  title     = {{Neural Heterogeneity and Efficient Population Codes for Communication Signals}},
  journal   = {Journal of Neurophysiology},
  year      = {2010},
  volume    = {104},
  number    = {5},
  pages     = {2543--2555},
  issn      = {0022-3077},
  abstract  = {Efficient sensory coding implies that populations of neurons should represent information-rich aspects of a signal with little redundancy. Recent studies have shown that neural heterogeneity in higher brain areas enhances the efficiency of encoding by reducing redundancy across the population. Here, we study how neural heterogeneity in the early stages of sensory processing influences the efficiency of population codes. Through the analysis of in vivo recordings, we contrast the encoding of two types of communication signals of electric fishes in the most peripheral sensory area of the CNS, the electrosensory lateral line lobe (ELL). We show that communication signals used during courtship (big chirps) and during aggressive encounters (small chirps) are encoded by different populations of ELL pyramidal cells, namely I-cells and E-cells, respectively. Most importantly, we show that the encoding strategy differs for the two signals and we argue that these differences allow these cell types to encode specifically information-rich features of the signals. Small chirps are detected, and their timing is accurately signaled through stereotyped spike bursts, whereas the shape of big chirps is accurately represented by variable increases in firing rate. Furthermore, we show that the heterogeneity across I-cells enhances the efficiency of the population code and thus permits the accurate discrimination of different quality courtship signals. Our study shows the importance of neural heterogeneity early in a sensory system and that it initiates the sparsification of sensory representation thereby contributing to the efficiency of the neural code.},
  doi       = {10.1152/jn.00256.2010},
  eprint    = {http://jn.physiology.org/content/104/5/2543.full.pdf},
  publisher = {American Physiological Society},
  url       = {http://jn.physiology.org/content/104/5/2543},
}

@Article{Miner_2016,
  author    = {Miner, D. and Triesch, J.},
  title     = {{Plasticity-Driven Self-Organization under Topological Constraints Accounts for Non-Random Features of Cortical Synaptic Wiring}},
  journal   = {PLoS Computational Biology},
  year      = {2016},
  owner     = {fschubert},
  timestamp = {2016.04.11},
}

@Article{Mizuseki_2013,
  author  = {Mizuseki, K. and Buzsáki, G.},
  title   = {{Preconfigured, skewed distribution of firing rates in the hippocampus and entorhinal cortex}},
  journal = {Cell Reports},
  year    = {2013},
  volume  = {4},
}

@Article{Naude_2013,
  author    = {Naud{\'e}, J. and Cessac, B. and Berry, H. and Delord, B.},
  title     = {{Effects of Cellular Homeostatic Intrinsic Plasticity on Dynamical and Computational Properties of Biological Recurrent Neural Networks}},
  journal   = {Journal of Neuroscience},
  year      = {2013},
  owner     = {fschubert},
  timestamp = {2017.01.27},
}

@Article{Nicola_2017,
  author    = {Nicola, W. and Clopath, C.},
  title     = {{Supervised learning in spiking neural networks with {FORCE} training}},
  journal   = {Nature Communications},
  year      = {2017},
  volume    = {8},
  number    = {1},
  month     = {dec},
  doi       = {10.1038/s41467-017-01827-3},
  publisher = {Springer Nature},
}

@Article{OConnor_2010,
  author    = {O'Connor, D. H. and Peron, S. P. and Huber, D. and Svoboda, K.},
  title     = {{Neural Activity in Barrel Cortex Underlying Vibrissa-Based Object Localization in Mice}},
  journal   = {Neuron},
  year      = {2010},
  volume    = {67},
  owner     = {fschubert},
  timestamp = {2017.02.15},
}

@Article{OReilly_2014,
  author    = {O'Reilly, R. C. and Wyatte, D. and Rohrlich, J.},
  title     = {{Learning Through Time in the Thamocortical Loops}},
  journal   = {arXiv:1407.3432},
  year      = {2014},
  owner     = {fschubert},
  timestamp = {2018.08.16},
}

@Article{Ostojic_2011,
  author    = {Ostojic, S.},
  title     = {{Interspike interval distributions of spiking neurons driven by fluctuating inputs}},
  journal   = {Journal of Neurophysiology},
  year      = {2011},
  volume    = {106},
  number    = {1},
  pages     = {361--373},
  issn      = {0022-3077},
  abstract  = {Interspike interval (ISI) distributions of cortical neurons exhibit a range of different shapes. Wide ISI distributions are believed to stem from a balance of excitatory and inhibitory inputs that leads to a strongly fluctuating total drive. An important question is whether the full range of experimentally observed ISI distributions can be reproduced by modulating this balance. To address this issue, we investigate the shape of the ISI distributions of spiking neuron models receiving fluctuating inputs. Using analytical tools to describe the ISI distribution of a leaky integrate-and-fire (LIF) neuron, we identify three key features: 1) the ISI distribution displays an exponential decay at long ISIs independently of the strength of the fluctuating input; 2) as the amplitude of the input fluctuations is increased, the ISI distribution evolves progressively between three types, a narrow distribution (suprathreshold input), an exponential with an effective refractory period (subthreshold but suprareset input), and a bursting exponential (subreset input); 3) the shape of the ISI distribution is approximately independent of the mean ISI and determined only by the coefficient of variation. Numerical simulations show that these features are not specific to the LIF model but are also present in the ISI distributions of the exponential integrate-and-fire model and a Hodgkin-Huxley-like model. Moreover, we observe that for a fixed mean and coefficient of variation of ISIs, the full ISI distributions of the three models are nearly identical. We conclude that the ISI distributions of spiking neurons in the presence of fluctuating inputs are well described by gamma distributions.},
  doi       = {10.1152/jn.00830.2010},
  eprint    = {http://jn.physiology.org/content/106/1/361.full.pdf},
  publisher = {American Physiological Society},
  url       = {http://jn.physiology.org/content/106/1/361},
}

@Article{Pape_1992,
  author    = {Pape, H.-C. and Mager, R.},
  title     = {{Nitric Oxide Controls Oscillatory Activity in Thalamocortical Neurons}},
  journal   = {Neuron},
  year      = {1992},
  volume    = {9},
  pages     = {441-448},
  owner     = {fschubert},
  timestamp = {2016.10.12},
}

@Article{Pfister_2006,
  author  = {Pfister, J.-P. and Toyoizumi, T. and Barber, D. and Gerstner, W.},
  title   = {{Optimal Spike-Timing-Dependent Plasticity for Precise Action Potential Firing in Supervised Learning}},
  journal = {Neural Computation},
  year    = {2006},
  volume  = {18},
  pages   = {1318--1348},
}

@Article{Philippides_2000,
  author    = {Philippides, A. and Husbands, P. and O{\textquoteright}Shea, M.},
  title     = {{Four-Dimensional Neuronal Signaling by Nitric Oxide: A Computational Analysis}},
  journal   = {Journal of Neuroscience},
  year      = {2000},
  volume    = {20},
  number    = {3},
  pages     = {1199--1207},
  issn      = {0270-6474},
  abstract  = {Nitric oxide (NO) is now recognized as a transmitter of neurons that express the neuronal isoform of the enzyme nitric oxide synthase. NO, however, violates some of the key tenets of chemical transmission, which is classically regarded as occurring at points of close apposition between neurons. It is the ability of NO to diffuse isotropically in aqueous and lipid environments that has suggested a radically different form of signaling in which the transmitter acts four-dimensionally in space and time, affecting volumes of the brain containing many neurons and synapses. Although {\textquotedblleft}volume signaling{\textquotedblright} clearly challenges simple connectionist models of neural processing, crucial to its understanding are the spatial and temporal dynamics of the spread of NO within the brain. Existing models of NO diffusion, however, have serious shortcomings because they represent solutions for {\textquotedblleft}point-sources,{\textquotedblright} which have no physical dimensions. Methods for overcoming these difficulties are presented here, and results are described that show how NO spreads from realistic neural architectures with both simple symmetrical and irregular shapes. By highlighting the important influence of the geometry of NO sources, our results provide insights into the four-dimensional spread of a diffusing messenger. We show for example that reservoirs of NO that accumulate in volumes of the nervous system where NO is not synthesized contribute significantly to the temporal and spatial dynamics of NO spread.},
  eprint    = {http://www.jneurosci.org/content/20/3/1199.full.pdf},
  publisher = {Society for Neuroscience},
  url       = {http://www.jneurosci.org/content/20/3/1199},
}

@Article{Philippides_2005,
  author    = {Philippides, A. and Ott, S. R. and Husbands, P. and Lovick, T. A. and O{\textquoteright}Shea, M.},
  title     = {{Modeling Cooperative Volume Signaling in a Plexus of Nitric Oxide Synthase-Expressing Neurons}},
  journal   = {Journal of Neuroscience},
  year      = {2005},
  volume    = {25},
  number    = {28},
  pages     = {6520--6532},
  issn      = {0270-6474},
  abstract  = {In vertebrate and invertebrate brains, nitric oxide (NO) synthase (NOS) is frequently expressed in extensive meshworks (plexuses) of exceedingly fine fibers. In this paper, we investigate the functional implications of this morphology by modeling NO diffusion in fiber systems of varying fineness and dispersal. Because size severely limits the signaling ability of an NO-producing fiber, the predominance of fine fibers seems paradoxical. Our modeling reveals, however, that cooperation between many fibers of low individual efficacy can generate an extensive and strong volume signal. Importantly, the signal produced by such a system of cooperating dispersed fibers is significantly more homogeneous in both space and time than that produced by fewer larger sources. Signals generated by plexuses of fine fibers are also better centered on the active region and less dependent on their particular branching morphology. We conclude that an ultrafine plexus is configured to target a volume of the brain with a homogeneous volume signal. Moreover, by translating only persistent regional activity into an effective NO volume signal, dispersed sources integrate neural activity over both space and time. In the mammalian cerebral cortex, for example, the NOS plexus would preferentially translate persistent regional increases in neural activity into a signal that targets blood vessels residing in the same region of the cortex, resulting in an increased regional blood flow. We propose that the fineness-dependent properties of volume signals may in part account for the presence of similar NOS plexus morphologies in distantly related animals.},
  doi       = {10.1523/JNEUROSCI.1264-05.2005},
  eprint    = {http://www.jneurosci.org/content/25/28/6520.full.pdf},
  publisher = {Society for Neuroscience},
  url       = {http://www.jneurosci.org/content/25/28/6520},
}

@Article{Queenan_2012,
  author    = {Queenan, B. N. and Lee, K. J. and Pak, D. T. S.},
  title     = {{Where Art Thou, Homeo(stasis)? Functional Diversity in Homeostatic Synaptic Plasticity}},
  journal   = {Neural Plasticity},
  year      = {2012},
  volume    = {2012},
  owner     = {fabian},
  timestamp = {2017.01.20},
}

@Article{Rajan_2006,
  author    = {Rajan, K. and Abbott, L. F.},
  title     = {{Eigenvalue Spectra of Random Matrices for Neural Networks}},
  journal   = {Physical Review Letters},
  year      = {2006},
  volume    = {97},
  number    = {18},
  month     = {nov},
  doi       = {10.1103/physrevlett.97.188104},
  publisher = {American Physical Society ({APS})},
}

@Article{Reber_1967,
  author  = {Reber, A. S.},
  title   = {{Implicit Learning of Artificial Grammars}},
  journal = {Journal of Verbal Learning and Verbal Behavior},
  year    = {1967},
}

@Article{vanRossum_2000,
  author    = {van Rossum, M. C. W. and Bi, G. Q. and Turrigiano, G. G.},
  title     = {{Stable Hebbian Learning from Spike Timing-Dependent Plasticity}},
  journal   = {Journal of Neuroscience},
  year      = {2000},
  volume    = {20},
  number    = {23},
  pages     = {8812--8821},
  issn      = {0270-6474},
  abstract  = {We explore a synaptic plasticity model that incorporates recent findings that potentiation and depression can be induced by precisely timed pairs of synaptic events and postsynaptic spikes. In addition we include the observation that strong synapses undergo relatively less potentiation than weak synapses, whereas depression is independent of synaptic strength. After random stimulation, the synaptic weights reach an equilibrium distribution which is stable, unimodal, and has positive skew. This weight distribution compares favorably to the distributions of quantal amplitudes and of receptor number observed experimentally in central neurons and contrasts to the distribution found in plasticity models without size-dependent potentiation. Also in contrast to those models, which show strong competition between the synapses, stable plasticity is achieved with little competition. Instead, competition can be introduced by including a separate mechanism that scales synaptic strengths multiplicatively as a function of postsynaptic activity. In this model, synaptic weights change in proportion to how correlated they are with other inputs onto the same postsynaptic neuron. These results indicate that stable correlation-based plasticity can be achieved without introducing competition, suggesting that plasticity and competition need not coexist in all circuits or at all developmental stages.},
  eprint    = {http://www.jneurosci.org/content/20/23/8812.full.pdf},
  publisher = {Society for Neuroscience},
  url       = {http://www.jneurosci.org/content/20/23/8812},
}

@Article{Roxin_2011,
  author    = {Roxin, A. and Brunel, N. and Hansel, D. and Mongillo, G. and van Vreeswijk, C.},
  title     = {{On the Distribution of Firing Rates in Networks of Cortical Neurons}},
  journal   = {Journal of Neuroscience},
  year      = {2011},
  volume    = {31},
  number    = {45},
  pages     = {16217--16226},
  issn      = {0270-6474},
  abstract  = {The distribution of in vivo average firing rates within local cortical networks has been reported to be highly skewed and long tailed. The distribution of average single-cell inputs, conversely, is expected to be Gaussian by the central limit theorem. This raises the issue of how a skewed distribution of firing rates might result from a symmetric distribution of inputs. We argue that skewed rate distributions are a signature of the nonlinearity of the in vivo f{\textendash}I curve. During in vivo conditions, ongoing synaptic activity produces significant fluctuations in the membrane potential of neurons, resulting in an expansive nonlinearity of the f{\textendash}I curve for low and moderate inputs. Here, we investigate the effects of single-cell and network parameters on the shape of the f{\textendash}I curve and, by extension, on the distribution of firing rates in randomly connected networks.},
  doi       = {10.1523/JNEUROSCI.1677-11.2011},
  eprint    = {http://jneurosci.org/content/31/45/16217.full.pdf},
  file      = {:/home/fabian/Uni/Neuro/papers/roxin_2011_firing_rates.pdf:PDF},
  owner     = {fabian},
  publisher = {Society for Neuroscience},
  timestamp = {2016.10.18},
  url       = {http://jneurosci.org/content/31/45/16217},
}

@InCollection{Rumelhart_1988,
  author    = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  title     = {{Neurocomputing: Foundations of Research}},
  publisher = {MIT Press},
  year      = {1988},
  editor    = {Anderson, James A. and Rosenfeld, Edward},
  chapter   = {Learning Representations by Back-propagating Errors},
  pages     = {696--699},
  address   = {Cambridge, MA, USA},
  isbn      = {0-262-01097-6},
  acmid     = {104451},
  numpages  = {4},
  url       = {http://dl.acm.org/citation.cfm?id=65669.104451},
}

@Article{Rumelhart_1985,
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  title  = {{Learning Internal Representations by Error Propagation}},
  year   = {1985},
}

@Article{Savin_2010,
  author    = {Savin, Cristina AND Joshi, Prashant AND Triesch, Jochen},
  title     = {{Independent Component Analysis in Spiking Neurons}},
  journal   = {PLOS Computational Biology},
  year      = {2010},
  volume    = {6},
  number    = {4},
  pages     = {1-10},
  month     = {04},
  abstract  = {Author Summary How the brain learns to encode and represent sensory information has been a longstanding question in neuroscience. Computational theories predict that sensory neurons should reduce redundancies between their responses to a given stimulus set in order to maximize the amount of information they can encode. Specifically, a powerful set of learning algorithms called Independent Component Analysis (ICA) and related models, such as sparse coding, have emerged as a standard for learning efficient codes for sensory information. These algorithms have been able to successfully explain several aspects of sensory representations in the brain, such as the shape of receptive fields of neurons in primary visual cortex. Unfortunately, it remains unclear how networks of spiking neurons can implement this function and, even more difficult, how they can learn to do so using known forms of neuronal plasticity. This paper solves this problem by presenting a model of a network of spiking neurons that performs ICA-like learning in a biologically plausible fashion, by combining three different forms of neuronal plasticity. We demonstrate the model's effectiveness on several standard sensory learning problems. Our results highlight the importance of studying the interaction of different forms of neuronal plasticity for understanding learning processes in the brain.},
  doi       = {10.1371/journal.pcbi.1000757},
  publisher = {Public Library of Science},
  url       = {http://dx.doi.org/10.1371%2Fjournal.pcbi.1000757},
}

@Article{Savin_2008,
  author    = {Savin, C. and Triesch, J. and Meyer-Hermann, M.},
  title     = {{Epileptogenesis due to glia-mediated synaptic scaling}},
  journal   = {Journal of the Royal Society Interface},
  year      = {2008},
  owner     = {fschubert},
  timestamp = {2017.02.22},
}

@Article{Schuez_1989,
  author    = {Sch\"uz, A. and Palm, G.},
  title     = {{Density of neurons and synapses in the cerebral cortex of the mouse}},
  journal   = {The Journal of Comparative Neurology},
  year      = {1989},
  volume    = {286},
  number    = {4},
  pages     = {442--455},
  issn      = {1096-9861},
  doi       = {10.1002/cne.902860404},
  keywords  = {connectivity, stereology, tissue shrinkage, excitation, cortical volume},
  publisher = {Alan R. Liss, Inc.},
  url       = {http://dx.doi.org/10.1002/cne.902860404},
}

@Article{Schrauwen_2008,
  author    = {Benjamin Schrauwen and Marion Wardermann and David Verstraeten and Jochen J. Steil and Dirk Stroobandt},
  title     = {{Improving reservoirs using intrinsic plasticity}},
  journal   = {Neurocomputing},
  year      = {2008},
  volume    = {71},
  number    = {7-9},
  pages     = {1159--1171},
  month     = {mar},
  doi       = {10.1016/j.neucom.2007.12.020},
  publisher = {Elsevier {BV}},
}

@Article{Shai_2015,
  author    = {Shai, A. S. and Anastassiou, C. A. and Larkum, M. E. and Koch, C.},
  title     = {{Physiology of Layer 5 Pyramidal Neurons in Mouse Primary Visual Cortex: Coincidence Detection through Bursting}},
  journal   = {PLOS Computational Biology},
  year      = {2015},
  volume    = {11},
  number    = {3},
  owner     = {fschubert},
  timestamp = {2018.01.11},
}

@Article{Siegert_1951,
  author    = {Siegert, A. J. F.},
  title     = {{On the First Passage Time Probability Problem}},
  journal   = {Phys. Rev.},
  year      = {1951},
  volume    = {81},
  pages     = {617--623},
  month     = {Feb},
  doi       = {10.1103/PhysRev.81.617},
  issue     = {4},
  numpages  = {0},
  publisher = {American Physical Society},
  url       = {http://link.aps.org/doi/10.1103/PhysRev.81.617},
}

@Article{Sjoestroem_2001,
  author    = {Sj\"ostr\"om, P. J. and Turrigiano, G. G.},
  title     = {{Rate, Timing, and Cooperativity Jointly Determine Cortical Synaptic Plasticity}},
  journal   = {Neuron},
  year      = {2001},
  owner     = {fschubert},
  timestamp = {2016.12.22},
}

@Article{Sjoestroem_2006,
  author    = {Sj\"oström, P. J. and H\"ausser, M.},
  title     = {{A Cooperative Switch Determines the Sign of Synaptic Plasticity in Distal Dendrites of Neocortical Pyramidal Neurons}},
  journal   = {Neuron},
  year      = {2006},
  volume    = {51},
  number    = {2},
  pages     = {227--238},
  month     = {jul},
  doi       = {10.1016/j.neuron.2006.06.017},
  publisher = {Elsevier {BV}},
}

@Article{Song_2000,
  author  = {Song, S. and Miller, K. D. and Abbott, L. F.},
  title   = {{Competitive Hebbian learning through spike-timing-dependent synaptic plasticity}},
  journal = {Nature Neuroscience},
  year    = {2000},
  volume  = {3},
  number  = {9},
  pages   = {919-926},
}

@Article{Song_2005,
  author    = {Song, S. and Sj\"ostr\"om, P. J. and Reigl, M. and Nelson, S. and Chklovskii, D. B.},
  title     = {{Highly Nonrandom Features of Synaptic Connectivity in Local Cortical Circuits}},
  journal   = {PLOS Biology},
  year      = {2005},
  volume    = {3},
  number    = {3},
  month     = {03},
  abstract  = {A dataset of hundreds of recordings in which four neurons were simultaneously monitored reveals clustered connectivity patterns among cortical neurons.},
  doi       = {10.1371/journal.pbio.0030068},
  keywords  = {log-normal},
  publisher = {Public Library of Science},
  url       = {http://dx.doi.org/10.1371%2Fjournal.pbio.0030068},
}

@Article{Spruston_2008,
  author    = {Spruston, N.},
  title     = {{Pyramidal neurons: dendritic structure and synaptic integration}},
  journal   = {Nature Reviews Neuroscience},
  year      = {2008},
  volume    = {9},
  number    = {3},
  pages     = {206--221},
  month     = {mar},
  doi       = {10.1038/nrn2286},
  publisher = {Springer Nature},
}

@Article{Statman_2014,
  author    = {Statman, A. and Kaufman, M. and Minerbi, A. and Ziv, N. E. and Brenner, N.},
  title     = {{Synaptic Size Dynamics as an Effectively Stochastic Process}},
  journal   = {PLOS Computational Biology},
  year      = {2014},
  volume    = {10},
  number    = {10},
  pages     = {1-17},
  month     = {10},
  abstract  = {Author Summary Synapses are specialized sites of cell–cell contact that serve to transmit signals between neurons and their targets, most commonly other neurons. It is widely believed that changes in synaptic properties, driven by prior activity or by other physiological signals, represent a major cellular mechanism by which neuronal networks are modified. Recent experiments show that in addition to directed changes, synaptic sizes also change spontaneously, with dynamics that seem to have strong stochastic components. In spite of these dynamics, however, population distributions of synaptic sizes are remarkably stable, and scale smoothly in response to various perturbations. In this study we show that fundamental aspects of synapse size dynamics are captured remarkably well by a simple statistical model known as the Kesten process: the random-like nature of synaptic size changes; the stability and shape of synaptic size distributions; their scaling following various perturbations; and the kinetics of new synapse formation. These findings indicate that the multiple microscopic processes involved in determining synaptic size combine in such a way that their collective behavior buffers many of the underlying details. The simplicity of the model and its robustness provide a new route for understanding the emergence of invariants at the level of the synaptic population.},
  doi       = {10.1371/journal.pcbi.1003846},
  publisher = {Public Library of Science},
  url       = {http://dx.doi.org/10.1371%2Fjournal.pcbi.1003846},
}

@Article{Steinert_2008,
  author  = {Steinert, J. and Kopp-Scheinpflug, C. and Baker, C. and Challiss, R. J. and Mistry, R. and Haustein, M. and Griffin, S. J. and Tong, H. and Graham, B. P. and Forsythe, I. D.},
  title   = {{Nitric Oxide Is a Volume Transmitter Regulating Postsynaptic Excitability at a Glutamatergic Synapse}},
  journal = {Neuron},
  year    = {2008},
  volume  = {60},
  number  = {4},
  pages   = {642 - 656},
  issn    = {0896-6273},
  doi     = {http://dx.doi.org/10.1016/j.neuron.2008.08.025},
  url     = {http://www.sciencedirect.com/science/article/pii/S0896627308007587},
}

@Article{Steinert_2011,
  author    = {Steinert, J. and Robinson, S. and Tong, H. and Haustein, M. and Kopp-Scheinpflug, C. and Forsythe, I.},
  title     = {{Nitric Oxide is an Activity-Dependent Regulator of Target Neuron Intrinsic Excitability}},
  journal   = {Neuron},
  year      = {2011},
  volume    = {71},
  number    = {2},
  pages     = {291-305},
  owner     = {fabian},
  timestamp = {2017.01.19},
}

@Article{Strong_1998,
  author  = {Strong, S. P. and Koberle, R. and {de Ruyter van Steveninck}, R. R. and Bialek, W.},
  title   = {{Entropy and Information in Neural Spike Trains}},
  journal = {Physical Review Letters},
  year    = {1998},
}

@Article{Sussillo_2009,
  author    = {Sussillo, D. and Abbott, L. F.},
  title     = {{Generating Coherent Patterns of Activity from Chaotic Neural Networks}},
  journal   = {Neuron},
  year      = {2009},
  volume    = {63},
  number    = {4},
  pages     = {544--557},
  month     = {aug},
  doi       = {10.1016/j.neuron.2009.07.018},
  publisher = {Elsevier {BV}},
}

@Article{Sweeney_2017,
  author   = {Sweeney, Y. and Clopath, C.},
  title    = {{Emergent spatial synaptic structure from diffusive plasticity}},
  journal  = {European Journal of Neuroscience},
  year     = {2017},
  volume   = {45},
  number   = {8},
  pages    = {1057--1067},
  issn     = {1460-9568},
  doi      = {10.1111/ejn.13279},
  keywords = {diffusive neurotransmitters, neural networks, synaptic connectivity, synaptic plasticity, volume transmission},
  url      = {http://dx.doi.org/10.1111/ejn.13279},
}

@Article{Sweeney_2015,
  author    = {Sweeney, Y. and Kotaleski, J. H. and Hennig, M. H.},
  title     = {{A Diffusive Homeostatic Signal Maintains Neural Heterogeneity and Responsiveness in Cortical Networks}},
  journal   = {PLoS Computational Biology},
  year      = {2015},
  file      = {:/home/fabian/Uni/Neuro/papers/Sweeney_Diff_Hom.PDF:PDF},
  owner     = {fschubert},
  timestamp = {2016.04.11},
}

@Misc{Matplotlib,
  author       = {Matplotlib Development Team},
  title        = {{Matplotlib (Version 1.5.3) [Computer Software]}},
  howpublished = {\url{https://matplotlib.org/1.5.3/index.html}},
  year         = {2016},
  owner        = {fschubert},
  timestamp    = {2016.10.06},
}

@Article{Tetzlaff_2012,
  author    = {Tetzlaff, C. and Kolodziejski, C. and Timme, M. and W\"org\"otter, F.},
  title     = {{Analysis of Synaptic Scaling in Combination with Hebbian Plasticity in Several Simple Networks}},
  journal   = {Frontiers in Computational Neuroscience},
  year      = {2012},
  owner     = {fschubert},
  timestamp = {2017.02.16},
}

@Article{Toyoizumi_2005,
  author    = {Toyoizumi, T. and Pfister, J.-P. and Aihara, K. and Gerstner, W.},
  title     = {{Generalized Bienenstock-Cooper-Munro rule for spiking neurons that maximizes information transmission}},
  journal   = {Proceedings of the National Academy of Sciences},
  year      = {2005},
  volume    = {102},
  file      = {:/home/fschubert/papers/Generalized_Bienenstock-Cooper-Munro_rule_for_spiking_neurons_that_maximizes_information_transmission_Toyoizumi_2005.pdf:PDF},
  owner     = {fschubert},
  timestamp = {2017.11.07},
}

@Article{Triesch_2007,
  author   = {Jochen Triesch},
  title    = {{Synergies Between Intrinsic and Synaptic Plasticity Mechanisms}},
  journal  = {Neural Computation},
  year     = {2007},
  volume   = {19},
  number   = {4},
  pages    = {885-909},
  note     = {PMID: 17348766},
  abstract = {We propose a model of intrinsic plasticity for a continuous activation model neuron based on information theory. We then show how intrinsic and synaptic plasticity mechanisms interact and allow the neuron to discover heavy-tailed directions in the input. We also demonstrate that intrinsic plasticity may be an alternative explanation for the sliding threshold postulated in the BCM theory of synaptic plasticity. We present a theoretical analysis of the interaction of intrinsic plasticity with different Hebbian learning rules for the case of clustered inputs. Finally, we perform experiments on the “bars” problem, a popular nonlinear independent component analysis problem.},
  doi      = {10.1162/neco.2007.19.4.885},
  eprint   = {https://doi.org/10.1162/neco.2007.19.4.885},
  file     = {:/home/fschubert/papers/Synergies_Between_Intrinsic_and_Synaptic_Plasticity_Mechanisms_Triesch_2007.pdf:PDF},
  url      = { 
 https://doi.org/10.1162/neco.2007.19.4.885
 
},
}

@Article{Tripathy_2013,
  author    = {Tripathy, S. J. and Padmanabhan, K. and Gerkin, R. C. and Urban, N. N.},
  title     = {{Intermediate intrinsic diversity enhances neural population coding}},
  journal   = {Proceedings of the National Academy of Sciences},
  year      = {2013},
  volume    = {110},
  number    = {20},
  pages     = {8248-8253},
  abstract  = {Cell-to-cell variability in molecular, genetic, and physiological features is increasingly recognized as a critical feature of complex biological systems, including the brain. Although such variability has potential advantages in robustness and reliability, how and why biological circuits assemble heterogeneous cells into functional groups is poorly understood. Here, we develop analytic approaches toward answering how neuron-level variation in intrinsic biophysical properties of olfactory bulb mitral cells influences population coding of fluctuating stimuli. We capture the intrinsic diversity of recorded populations of neurons through a statistical approach based on generalized linear models. These models are flexible enough to predict the diverse responses of individual neurons yet provide a common reference frame for comparing one neuron to the next. We then use Bayesian stimulus decoding to ask how effectively different populations of mitral cells, varying in their diversity, encode a common stimulus. We show that a key advantage provided by physiological levels of intrinsic diversity is more efficient and more robust encoding of stimuli by the population as a whole. However, we find that the populations that best encode stimulus features are not simply the most heterogeneous, but those that balance diversity with the benefits of neural similarity.},
  doi       = {10.1073/pnas.1221214110},
  eprint    = {http://www.pnas.org/content/110/20/8248.full.pdf},
  owner     = {fschubert},
  timestamp = {2017.03.21},
  url       = {http://www.pnas.org/content/110/20/8248.abstract},
}

@Article{Turrigiano_2011,
  author    = {Turrigiano, G. G.},
  title     = {{Too Many Cooks? Intrinsic and Synaptic Homeostatic Mechanisms in Cortical Circuit Refinement}},
  journal   = {Annual Review of Neuroscience},
  year      = {2011},
  owner     = {fabian},
  timestamp = {2017.01.28},
}

@Article{Turrigiano_2008,
  author    = {Turrigiano, G. G.},
  title     = {{The Self-Tuning Neuron: Synaptic Scaling of Excitatory Synapses}},
  journal   = {Cell},
  year      = {2008},
  volume    = {135},
  owner     = {fschubert},
  timestamp = {2017.02.16},
}

@Article{Turrigiano_1994,
  author    = {Turrigiano, G. G. and Abbott, L. F. and Marder, E.},
  title     = {{Activity-dependent changes in the intrinsic properties of cultured neurons}},
  journal   = {Science},
  year      = {1994},
  volume    = {264},
  number    = {5161},
  pages     = {974--977},
  issn      = {0036-8075},
  abstract  = {Learning and memory arise through activity-dependent modifications of neural circuits. Although the activity dependence of synaptic efficacy has been studied extensively, less is known about how activity shapes the intrinsic electrical properties of neurons. Lobster stomatogastric ganglion neurons fire in bursts when receiving synaptic and modulatory input but fire tonically when pharmacologically isolated. Long-term isolation in culture changed their intrinsic activity from tonic firing to burst firing. Rhythmic stimulation reversed this transition through a mechanism that was mediated by a rise in intracellular calcium concentration. These data suggest that neurons regulate their conductances to maintain stable activity patterns and that the intrinsic properties of a neuron depend on its recent history of activation.},
  doi       = {10.1126/science.8178157},
  eprint    = {http://science.sciencemag.org/content/264/5161/974.full.pdf},
  publisher = {American Association for the Advancement of Science},
  url       = {http://science.sciencemag.org/content/264/5161/974},
}

@Article{Urbanczik_2014,
  author    = {Urbanczik, R. and Senn, W.},
  title     = {{Learning by the Dendritic Prediction of Somatic Spiking}},
  journal   = {Neuron},
  year      = {2014},
  volume    = {81},
  number    = {3},
  pages     = {521--528},
  month     = {feb},
  doi       = {10.1016/j.neuron.2013.11.030},
  publisher = {Elsevier {BV}},
}

@Article{Vogels_2011,
  author    = {Vogels, T. P. and Sprekeler, H. and Zenke, F. and Clopath, C. and Gerstner, W.},
  title     = {{Inhibitory Plasticity Balances Excitation and Inhibition in Sensory Pathways and Memory Networks}},
  journal   = {Science},
  year      = {2011},
  volume    = {334},
  number    = {6062},
  pages     = {1569--1573},
  issn      = {0036-8075},
  abstract  = {Cortical neurons receive balanced excitatory and inhibitory synaptic currents. Such a balance could be established and maintained in an experience-dependent manner by synaptic plasticity at inhibitory synapses. We show that this mechanism provides an explanation for the sparse firing patterns observed in response to natural stimuli and fits well with a recently observed interaction of excitatory and inhibitory receptive field plasticity. The introduction of inhibitory plasticity in suitable recurrent networks provides a homeostatic mechanism that leads to asynchronous irregular network states. Further, it can accommodate synaptic memories with activity patterns that become indiscernible from the background state but can be reactivated by external stimuli. Our results suggest an essential role of inhibitory plasticity in the formation and maintenance of functional cortical circuitry.},
  doi       = {10.1126/science.1211095},
  eprint    = {http://science.sciencemag.org/content/334/6062/1569.full.pdf},
  publisher = {American Association for the Advancement of Science},
  url       = {http://science.sciencemag.org/content/334/6062/1569},
}

@Article{Vreeswijk_1998,
  author    = {van Vreeswijk, C. and Sompolinsky, H.},
  title     = {{Chaotic Balanced State in a Model of Cortical Circuits}},
  journal   = {Neural Computation},
  year      = {1998},
  owner     = {fschubert},
  timestamp = {2017.01.17},
}

@Article{Vreeswijk_1996,
  author   = {van Vreeswijk, C. and Sompolinsky, H.},
  title    = {{Chaos in neuronal networks with balanced excitatory and inhibitory activity.}},
  journal  = {Science},
  year     = {1996},
  volume   = {274},
  pages    = {1724--1726},
  abstract = {Neurons in the cortex of behaving animals show temporally irregular spiking patterns. The origin of this irregularity and its implications for neural processing are unknown. The hypothesis that the temporal variability in the firing of a neuron results from an approximate balance between its excitatory and inhibitory inputs was investigated theoretically. Such a balance emerges naturally in large networks of excitatory and inhibitory neuronal populations that are sparsely connected by relatively strong synapses. The resulting state is characterized by strongly chaotic dynamics, even when the external inputs to the network are constant in time. Such a network exhibits a linear response, despite the highly nonlinear dynamics of single neurons, and reacts to changing external stimuli on time scales much smaller than the integration time constant of a single neuron.},
  keywords = {Animals; Cerebral Cortex; Haplorhini; Models,Neurological; Nerve Net; Neurons; Nonlinear Dynami},
  pmid     = {8939866},
}

@Article{Wainrib_2016,
  author    = {Gilles Wainrib and Mathieu N. Galtier},
  title     = {{A local Echo State Property through the largest Lyapunov exponent}},
  journal   = {Neural Networks},
  year      = {2016},
  volume    = {76},
  pages     = {39 - 45},
  issn      = {0893-6080},
  abstract  = {Echo State Networks are efficient time-series predictors, which highly depend on the value of the spectral radius of the reservoir connectivity matrix. Based on recent results on the mean field theory of driven random recurrent neural networks, enabling the computation of the largest Lyapunov exponent of an ESN, we develop a cheap algorithm to establish a local and operational version of the Echo State Property.},
  doi       = {https://doi.org/10.1016/j.neunet.2015.12.013},
  keywords  = {Reservoir computing, Mean field theory, Lyapunov exponents, Echo State Networks},
  owner     = {fschubert},
  timestamp = {2019.02.12},
  url       = {http://www.sciencedirect.com/science/article/pii/S0893608015002828},
}

@Article{Wei_2016,
  author   = {Wei, X.-X. and Stocker, A. A.},
  title    = {{Mutual Information, Fisher Information, and Efficient Coding}},
  journal  = {Neural Computation},
  year     = {2016},
  volume   = {28},
  number   = {2},
  pages    = {305-326},
  note     = {PMID: 26654209},
  abstract = {Fisher information is generally believed to represent a lower bound on mutual information (Brunel \& Nadal, 1998), a result that is frequently used in the assessment of neural coding efficiency. However, we demonstrate that the relation between these two quantities is more nuanced than previously thought. For example, we find that in the small noise regime, Fisher information actually provides an upper bound on mutual information. Generally our results show that it is more appropriate to consider Fisher information as an approximation rather than a bound on mutual information. We analytically derive the correspondence between the two quantities and the conditions under which the approximation is good. Our results have implications for neural coding theories and the link between neural population coding and psychophysically measurable behavior. Specifically, they allow us to formulate the efficient coding problem of maximizing mutual information between a stimulus variable and the response of a neural population in terms of Fisher information. We derive a signature of efficient coding expressed as the correspondence between the population Fisher information and the distribution of the stimulus variable. The signature is more general than previously proposed solutions that rely on specific assumptions about the neural tuning characteristics. We demonstrate that it can explain measured tuning characteristics of cortical neural populations that do not agree with previous models of efficient coding.},
  doi      = {10.1162/NECO\_a\_00804},
  eprint   = {https://doi.org/10.1162/NECO_a_00804},
  file     = {:/home/fschubert/papers/Mutual_Information_Fisher_Information_and_Efficient_Coding_Wei_Stocker.pdf:PDF},
  url      = { 
 https://doi.org/10.1162/NECO_a_00804
 
},
}

@Article{Wohrer_2012,
  author    = {Wohrer, A. and Humphries, M. D. and Machens, C.},
  title     = {{Population-wide distributions of neural activity during perceptual decision-making}},
  journal   = {Progress in Neurobiology},
  year      = {2012},
  owner     = {fabian},
  timestamp = {2016.12.11},
}

@Article{Yassin_2010,
  author  = {Yassin, L. and Benedetti, B. L. and Jouhanneau, J.-S. and Wen, J. A. and Poulet, J. F. and Barth, A. L.},
  title   = {{An Embedded Subnetwork of Highly Active Neurons in the Neocortex}},
  journal = {Neuron},
  year    = {2010},
  volume  = {68},
  number  = {6},
  pages   = {1043 - 1050},
  issn    = {0896-6273},
  doi     = {http://dx.doi.org/10.1016/j.neuron.2010.11.029},
  url     = {http://www.sciencedirect.com/science/article/pii/S0896627310009712},
}

@Article{Yasumatsu_2008,
  author    = {Yasumatsu, N. and Matsuzaki, M. and Miyazaki, T. and Noguchi, J. and Kasai, H.},
  title     = {{Principles of Long-Term Dynamics of Dendritic Spines}},
  journal   = {Journal of Neuroscience},
  year      = {2008},
  volume    = {28},
  number    = {50},
  pages     = {13592--13608},
  issn      = {0270-6474},
  abstract  = {Long-term potentiation of synapse strength requires enlargement of dendritic spines on cerebral pyramidal neurons. Long-term depression is linked to spine shrinkage. Indeed, spines are dynamic structures: they form, change their shapes and volumes, or can disappear in the space of hours. Do all such changes result from synaptic activity, or do some changes result from intrinsic processes? How do enlargement and shrinkage of spines relate to elimination and generation of spines, and how do these processes contribute to the stationary distribution of spine volumes? To answer these questions, we recorded the volumes of many individual spines daily for several days using two-photon imaging of CA1 pyramidal neurons in cultured slices of rat hippocampus between postnatal days 17 and 23. With normal synaptic transmission, spines often changed volume or were created or eliminated, thereby showing activity-dependent plasticity. However, we found that spines changed volume even after we blocked synaptic activity, reflecting a native instability of these small structures over the long term. Such {\textquotedblleft}intrinsic fluctuations{\textquotedblright} showed unique dependence on spine volume. A mathematical model constructed from these data and the theory of random fluctuations explains population behaviors of spines, such as rates of elimination and generation, stationary distribution of volumes, and the long-term persistence of large spines. Our study finds that generation and elimination of spines are more prevalent than previously believed, and spine volume shows significant correlation with its age and life expectancy. The population dynamics of spines also predict key psychological features of memory.},
  doi       = {10.1523/JNEUROSCI.0603-08.2008},
  eprint    = {http://www.jneurosci.org/content/28/50/13592.full.pdf},
  keywords  = {log-normal},
  publisher = {Society for Neuroscience},
  url       = {http://www.jneurosci.org/content/28/50/13592},
}

@Article{Yi_2017,
  author    = {Yi, G. and Wang, J. and Wei, X. and Deng, B.},
  title     = {{Action potential initiation in a two-compartment model of pyramidal neuron mediated by dendritic $\mathrm{Ca}^{2+}$ spike}},
  journal   = {Scientific Reports},
  year      = {2017},
  volume    = {7},
  number    = {1},
  month     = {apr},
  doi       = {10.1038/srep45684},
  publisher = {Springer Nature},
}

@Article{Yildiz_2012,
  author    = {Izzet B. Yildiz and Herbert Jaeger and Stefan J. Kiebel},
  title     = {{Re-visiting the echo state property}},
  journal   = {Neural Networks},
  year      = {2012},
  volume    = {35},
  pages     = {1 - 9},
  issn      = {0893-6080},
  abstract  = {An echo state network (ESN) consists of a large, randomly connected neural network, the reservoir, which is driven by an input signal and projects to output units. During training, only the connections from the reservoir to these output units are learned. A key requisite for output-only training is the echo state property (ESP), which means that the effect of initial conditions should vanish as time passes. In this paper, we use analytical examples to show that a widely used criterion for the ESP, the spectral radius of the weight matrix being smaller than unity, is not sufficient to satisfy the echo state property. We obtain these examples by investigating local bifurcation properties of the standard ESNs. Moreover, we provide new sufficient conditions for the echo state property of standard sigmoid and leaky integrator ESNs. We furthermore suggest an improved technical definition of the echo state property, and discuss what practicians should (and should not) observe when they optimize their reservoirs for specific tasks.},
  doi       = {https://doi.org/10.1016/j.neunet.2012.07.005},
  file      = {:/home/fschubert/papers/Yildiz_Jaeger_Kiebel-Re-visiting_the_echo_state_property.pdf:PDF},
  keywords  = {Echo state network, Spectral radius, Bifurcation, Diagonally Schur stable, Lyapunov},
  owner     = {fschubert},
  timestamp = {2019.02.12},
  url       = {http://www.sciencedirect.com/science/article/pii/S0893608012001852},
}

@Article{Zenke_2017,
  author    = {Zenke, F. and Gerstner, W.},
  title     = {{Hebbian plasticity requires compensatory processes on multiple timescales}},
  journal   = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  year      = {2017},
  volume    = {372},
  file      = {:/home/fschubert/papers/Hebbian_plasticity_requires_compensatory_processes_on_multiple_timescales.pdf:PDF},
  owner     = {fschubert},
  timestamp = {2017.11.01},
}

@Article{Zhang_1998,
  author    = {Zhang, L. I. and Tao, H. W. and Holt, C. E. and Harris, W. A. and Poo, M.},
  title     = {{A critical window for cooperation and competition among developing retinotectal synapses}},
  journal   = {Nature},
  year      = {1998},
  volume    = {395},
  pages     = {37-44},
  owner     = {fschubert},
  timestamp = {2016.10.06},
}

@Article{Pengsheng_2013,
  author    = {Zheng, P. and Dimitrakakis, C. and Triesch, J.},
  title     = {{Network Self-Organization Explains the Statistics and Dynamics of Synaptic Connection Strength in Cortex}},
  journal   = {PLOS Computational Biology},
  year      = {2013},
  owner     = {fschubert},
  timestamp = {2016.12.20},
}

@Book{Dyn_Sys_Hirsch,
  title     = {{Differential Equations, Dynamical Systems, and Linear Algebra}},
  publisher = {Elsevier},
  year      = {1974},
  editor    = {Hirsch, M. W. and Smale, S.},
  volume    = {60},
  series    = {Pure and Applied Mathematics},
  owner     = {fabian},
  timestamp = {2016.10.18},
}

@Article{Murray_2019,
  author  = {Murray, James M},
  title   = {{Local online learning in recurrent networks with random feedback}},
  journal = {eLIFE},
  year    = {2019},
}

@Article{Hesse2014,
  author  = {Hesse, J. and Gross, T.},
  title   = {{Self-organized criticality as a fundamental property of neural systems}},
  journal = {Frontier in Systems Neuroscience},
  year    = {2014},
  volume  = {8},
  number  = {166},
}

@Article{Whittington_2019,
  author  = {Whittington, James C.R. and Bogacz, Rafal},
  title   = {{Theories of Error Back-Propagation in the Brain}},
  journal = {Trends in Cognitive Sciences},
  year    = {2019},
  volume  = {23},
  number  = {3},
  pages   = {235-250},
  month   = mar,
}

@Misc{Alstott2017,
  author        = {Alstott, J.},
  title         = {{powerlaw: A Python Package for Analysis of Heavy-Tailed Distributions (Version 1.4.1) [Computer Software]}},
  howpublished  = {\url{https://pypi.python.org/pypi/powerlaw}},
  year          = {2017},
  __markedentry = {[fschubert:]},
  owner         = {fschubert},
  timestamp     = {2016.10.06},
}

@Misc{Brette2016,
  author        = {Brette, Romain and Goodman, Dan and Stirnberg, Marcel},
  title         = {{The Brian spiking neural network simulator (Version 1.0) [Computer Software]}},
  howpublished  = {\url{http://www.briansimulator.org/}},
  year          = {2016},
  __markedentry = {[fschubert:]},
  owner         = {fschubert},
  timestamp     = {2016.10.06},
}

@Misc{Team2016,
  author        = {Matplotlib Development Team},
  title         = {{Matplotlib (Version 1.5.3) [Computer Software]}},
  howpublished  = {\url{https://matplotlib.org/1.5.3/index.html}},
  year          = {2016},
  __markedentry = {[fschubert:]},
  owner         = {fschubert},
  timestamp     = {2016.10.06},
}

@Misc{Alstott2017a,
  author        = {Alstott, J.},
  title         = {{powerlaw: A Python Package for Analysis of Heavy-Tailed Distributions (Version 1.4.1) [Computer Software]}},
  howpublished  = {\url{https://pypi.python.org/pypi/powerlaw}},
  year          = {2017},
  __markedentry = {[fschubert:6]},
  owner         = {fschubert},
  timestamp     = {2016.10.06},
}

@Misc{Brette2016a,
  author        = {Brette, Romain and Goodman, Dan and Stirnberg, Marcel},
  title         = {{The Brian spiking neural network simulator (Version 1.0) [Computer Software]}},
  howpublished  = {\url{http://www.briansimulator.org/}},
  year          = {2016},
  __markedentry = {[fschubert:6]},
  owner         = {fschubert},
  timestamp     = {2016.10.06},
}

@Misc{Team2016a,
  author        = {Matplotlib Development Team},
  title         = {{Matplotlib (Version 1.5.3) [Computer Software]}},
  howpublished  = {\url{https://matplotlib.org/1.5.3/index.html}},
  year          = {2016},
  __markedentry = {[fschubert:6]},
  owner         = {fschubert},
  timestamp     = {2016.10.06},
}

@Article{Bellec_2019,
  author        = {Guillaume Bellec and Franz Scherr and Elias Hajek and Darjan Salaj and Robert Legenstein and Wolfgang Maass},
  title         = {{Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets}},
  year          = {2019},
  __markedentry = {[fschubert:6]},
  abstract      = {The way how recurrently connected networks of spiking neurons in the brain acquire powerful information processing capabilities through learning has remained a mystery. This lack of understanding is linked to a lack of learning algorithms for recurrent networks of spiking neurons (RSNNs) that are both functionally powerful and can be implemented by known biological mechanisms. Since RSNNs are simultaneously a primary target for implementations of brain-inspired circuits in neuromorphic hardware, this lack of algorithmic insight also hinders technological progress in that area. The gold standard for learning in recurrent neural networks in machine learning is back-propagation through time (BPTT), which implements stochastic gradient descent with regard to a given loss function. But BPTT is unrealistic from a biological perspective, since it requires a transmission of error signals backwards in time and in space, i.e., from post- to presynaptic neurons. We show that an online merging of locally available information during a computation with suitable top-down learning signals in real-time provides highly capable approximations to BPTT. For tasks where information on errors arises only late during a network computation, we enrich locally available information through feedforward eligibility traces of synapses that can easily be computed in an online manner. The resulting new generation of learning algorithms for recurrent neural networks provides a new understanding of network learning in the brain that can be tested experimentally. In addition, these algorithms provide efficient methods for on-chip training of RSNNs in neuromorphic hardware.},
  date          = {2019-01-25},
  eprint        = {1901.09049v2},
  eprintclass   = {cs.NE},
  eprinttype    = {arXiv},
  file          = {online:http\://arxiv.org/pdf/1901.09049v2:PDF},
  keywords      = {cs.NE},
}

@Comment{jabref-meta: databaseType:bibtex;}
